# Outracing champion Gran Turismo drivers with deep reinforcement learning

### 1. 핵심 주장 및 주요 기여 요약

**Gran Turismo Sophy (GT Sophy)**는 심층 강화학습을 이용해 세계 최고 수준의 그란투리스모 드라이버들을 이기는 첫 번째 AI 에이전트입니다. 본 연구의 핵심 주장은 다음과 같습니다:[1]

**주요 기여**:
- **모델 프리 강화학습의 고도화**: 상태 기반의 값 함수를 통해 복잡한 동역학 환경에서 최고 성능 달성[1]
- **새로운 알고리즘 개발**: QR-SAC(Quantile Regression Soft Actor-Critic) 알고리즘으로 N-step 리턴과 분포적 가치 추정 도입[1]
- **혼합 시나리오 훈련**: 인간이 정의한 중요한 레이싱 상황들에 대한 체계적 노출로 강건한 기술 습득[1]
- **인간 규범 학습**: 명확하지 않은 스포츠맨십 규칙을 보상 함수에 인코딩하여 에이전트가 인간과 상호작용 가능[1]

***

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 2.1 해결하고자 하는 문제

논문은 다음 네 가지 도전 과제를 제시합니다:

1. **레이스카 제어**: 마찰 한계에서의 비선형 동역학 제어
2. **레이싱 전술**: 정밀한 추월 및 방어 기동
3. **레이싱 에티켓**: 주관적이고 문맥 의존적인 스포츠맨십 규칙 준수
4. **레이싱 전략**: 상대방 모델링 및 의사결정

**기존 접근의 한계**:
- 순수 자기 플레이(self-play)는 인간의 부정확성에 대비 부족[1]
- 사전 계산 궤적과 모델 예측 제어는 모델 오류에 취약[1]
- 특정 상황(예: 슬립스트림)의 노출 부족 ('노출 문제')[1]

#### 2.2 제안 방법 및 수식

**QR-SAC 알고리즘**[1]

QR-SAC는 다음 손실 함수를 사용합니다:

$$L_Q = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim D} \left[ \sum_{j=1}^{M} \rho_j \left( y_i - Z^{\hat{\theta}}_j(s_t, a_t) \right) \right]^2$$

여기서:
- $\rho_j$: quantile Huber 손실 함수
- $Z^{\hat{\theta}}_j$: i번째 quantile의 값 함수
- M: quantile 개수 (M=32)

**정책 손실 함수**:

$$J_\pi(\phi) = \mathbb{E}_{s \sim D, a \sim \pi_\phi} \left[ \min_{i \in \{1,2\}} Q_i(s, a) - \alpha \log \pi_\phi(a|s) \right]$$

여기서 $\alpha$: 엔트로피 온도 ($\alpha = 0.01$)

**N-step 리턴 타겟**:

$$y_i = r_t + \sum_{t'=1}^{N-1} \gamma^{t'} r_{t+t'} + \gamma^N Z^{\hat{\theta}}_k(s_{t+N}, a) + \alpha \log \pi(a|s_{t+N})$$

여기서 $\gamma = 0.9896$ (할인계수)

**보상 함수 설계**[1]

다중 성분 선형 조합:

$$R(s, s') = w_{cp} R_{cp} + w_{soc} R_{soc} + w_w R_w + w_{ts} R_{ts} + w_{ps} R_{ps} + w_c R_c + w_r R_r + w_{uc} R_{uc}$$

각 성분:

1. **과정 진행 보상**: $R_{cp}(s, s') = l(s') - l(s)$
   - $l$: 중심선 상의 진행도

2. **오프코스 패널티**: $R_{soc}(s, s') = -k_{soc} \cdot (s_o)^2 \cdot s_{kph}^2$
   - $s_o$: 누적 오프코스 시간
   - $s_{kph}$: 시속(km/h)

3. **벽 충돌 패널티**: $R_w(s, s') = -k_w \cdot (s_w)^2 \cdot s_{kph}^2$

4. **타이어 슬립 패널티**: 

```math
R_{ts}(s, s') = -\sum_{i=1}^{4} \min(1.0, \max(\text{tsr}\_i, \text{ts}\_\text{angle}_i))
```


5. **추월 보상**: 

```math
R_{ps}(s, s') = \sum_i \max\left( \mathbb{1}_{b,f}(x) \text{ or } \mathbb{1}_{b,f}(x'), \frac{L_i' - L_i}{f+b} \right)
```

- $b = 20$ m (뒤쪽 범위), $f = 40$ m (앞쪽 범위)

6. **충돌 페널티**: 

```math
R_c(s, s') = -\sum_i \max(s_{c,i})
```

7. **후방 충돌 페널티**: 

$$R_r(s, s') = -\mathbb{1}_{c,i} \cdot \mathbb{1}_{l_i \text{ in front}} \cdot \left(\frac{\|s_v\|^2 + \|s_{v,i}\|^2}{2}\right)$$

8. **부정당한 충돌 페널티**:

```math
R_{uc}(s, s') = -\mathbb{1}_{u_{s,i}}
```

#### 2.3 모델 구조

**신경망 아키텍처**:[1]
- 은닉층 4개, 각 2,048 개 유닛
- 활성화 함수: ReLU
- Dropout (확률=0.1) 적용
- 정책 네트워크: 2차원 스쿼시된 정규분포 출력 (가속도/제동, 조향)
- Q-함수 네트워크: 32개 quantile 예측

**입력 특성**:[1]
- 차량 상태: 3D 속도, 각속도, 가속도, 타이어 부하, 슬립각
- 트랙 정보: 60개 3D 포인트 (진행도 기반 인코딩)
- 대상 차량 정보: 앞/뒤로 정렬된 상대 위치, 속도, 가속도

**출력**:
- 액션 공간: 2차원 연속값 $a \in [-1, 1]^2$

#### 2.4 성능 향상

**시간 시련 성능**:[1]
- Seaside: GT Sophy 107.964s vs 최고 인간 선수 (Emily Jones) 시간 대비 0.547s 우수
- Maggiore: GT Sophy 114.249s vs 최고 인간 (Valerio Gallo) 114.466s 우수
- Sarthe: GT Sophy 193.080s vs 최고 인간 (Igor Fraga) 194.888s 우수

**팀 경기 성적**:[1]
- 2021년 7월: 인간팀 86-70 승리
- 2021년 10월: GT Sophy팀 104-52 승리 (두 배 이상의 점수 차이)

**학습 효율**:[1]
- 처음 1-2일: 인간의 95% 수준 달성
- 10-25일: 최고 인간 수준 도달
- 총 45,000시간 이상의 드라이빙 시뮬레이션

#### 2.5 한계

**인식 차이**:
- GT Sophy: 정확한 x, y, z 트랙 맵 정보
- 인간: 시각적 인지 (커브, 노면 재질 감지 불가)

**상대 정보**:
- GT Sophy: 정확한 위치, 속도, 가속도 (점 표현)
- 인간: 전체 차량 시각, 후방 미러/대기 필요

**차량 제어**:
- 수동 변속, 트랙션 컨트롤 미지원
- 액션 빈도: GT Sophy 10Hz vs 인간 60Hz 제어

**반응 시간**:[1]
- GT Sophy: ~23-30ms (비동기 통신 + 추론)
- 전문 선수: 200-250ms
- 인위적 지연 실험으로 100-250ms에서도 초인간 성능 유지

**전략 부족**:
- 좋은 추월 기회를 놓칠 수 있음
- 높은 페널티 상황에서도 강하게 추월 시도
- 슬립스트림 복귀 가능 상황에서 부정확한 판단

***

### 3. 일반화 성능 향상 가능성 분석

#### 3.1 현재 논문의 일반화 전략

**혼합 시나리오 훈련**:[1]
- 고정 curriculum 없이 상호 보완적 시나리오 학습
- 다중 표 계층화 샘플링으로 기술 유지
- "노출 문제" 해결: PID 컨트롤러로 특정 궤적 구성

**대상 대상화된 상황**:
- 8대 그리드 스타트
- 슬립스트림 추월 (직선도로)
- 최종 chicane 마스터링

**강건성 향상**:
- 미다양한 대상 AI 혼합 (built-in AI + 이전 에이전트)
- 실수 학습: 제어 손실 이전 상태 저장 후 재훈련
- 정책 선택 프로세스: 다중 목표 최적화 (라운드 로빈 경합, 인간 심사)

#### 3.2 최신 일반화 연구 기반 향상 가능성

**1. Vision 기반 강화학습**[2]
현재 GT Sophy는 전역 특징(위치 정보)을 사용하지만, 최신 연구는 카메라 픽셀 입력 기반 초인간 성능을 달성했습니다. 이는 다음 장점을 제공:[2]
- 실제 자율주행 이동성 높음
- 시각적 도메인 무작위화로 일반화 향상

**2. 정책 커스터마이제이션**[3]
Residual-MPPI 알고리즘이 GT Sophy 1.0을 온라인으로 적응 가능하게 했습니다. 이를 통해:[3]
- 새로운 환경(경로, 차량)에 빠른 적응
- 소수(few-shot) 또는 제로샷(zero-shot) 학습 가능

**3. 도메인 무작위화 및 비시뮬레이션**[4]
최신 드론 레이싱 연구(Swift)는 다음 기법을 사용:[4]
- 동역학 모델 무작위화
- 인식 노이즈 모델링
- 실제 비행 3-4회 미세조정으로 현실적응 달성

이를 GT Sophy에 적용 시 **sim-to-real 격차 감소** 가능

**4. 분포 심사(Bisimulation) 방법**[5]
Deep Policy Similarities (DeePS)는 시간적 의존성을 고려한 행동 유사성 기반 상태 표현을 학습합니다. 이는:[5]
- 다양한 환경(절차적 생성)에서 일반화 개선
- 기존 domain randomization 이상의 성능

**5. 하이브리드 접근법**[6][7]
- 모델 기반 방법(MPC)과 모델 프리 RL 결합
- Imitation Learning (전문가 데모) 통합으로 수렴 가속

#### 3.3 GT Sophy의 일반화 한계 및 개선 방안

**현재 한계**:
1. 단일 게임 엔진 종속성
2. 사전 정의된 트랙 정보 의존
3. 특정 인간 드라이버 분포에만 최적화
4. 규칙 기반 스포츠맨십 인코딩의 경직성

**개선 가능 방향**:
| 측면 | 현재 상태 | 가능한 개선 |
|------|---------|-----------|
| **입력 형식** | 전역 좌표 + 특성 | 카메라 픽셀 + LiDAR 시뮬레이션 |
| **학습 패러다임** | 모델 프리 RL | 자기 지도 학습 + 대조 학습 |
| **도메인 확장** | GT Sport만 | 다양한 시뮬레이터 (Carla, IPG) |
| **인간 적응** | 고정 규칙 | 메타 학습 기반 온라인 적응 |
| **현실 이전** | 없음 | 작은 실제 차량 테스트 (예: 드론처럼) |

***

### 4. 앞으로의 연구 영향 및 고려사항

#### 4.1 학계 및 산업계 영향

**학술적 기여**:[8]
- **2022 ACM SIGAI Industry Award** 수상: 연속 제어에서의 RL 적용성 입증
- RL이 게임 AI를 넘어 **실제 제어 시스템**으로의 가능성 제시
- 인간 규범 학습의 새로운 패러다임 제공

**산업 응용**:[8][1]
1. **자율주행**: 극한 상황(고속 추월, 정밀 제어)에서의 의사결정 기술
2. **시뮬레이션 기반 훈련**: 전문 드라이버 양성의 저비용 대안
3. **게임 AI**: GT 7에 GT Sophy 2.1 통합 (2025년 3월 출시)[9]
   - 플레이어의 스킬 수준별 맞춤형 난이도
   - 19개 경로/차량 선택 가능
   - 타이어/연료 소비 커스터마이징

#### 4.2 최신 연구 트렌드 (2023-2025)

**1. 멀티모달 입력 통합**[2]
- Vision 기반 GT 에이전트 개발으로 국지적 센서만 사용
- 현실성 증대

**2. 자율 테스트 드라이버**[6]
- GT 기술을 레이스카 개발에 적용
- 차량 설정 최적화 자동화

**3. 정책 적응 및 메타 러닝**[3]
- 온라인 정책 커스터마이제이션 가능
- 다양한 환경에 빠른 적응

**4. Curriculum과 수정적 학습**[10][11]
- 단계적 학습보다 혼합 시나리오의 효과성 재확인
- 다양한 상대방 분포의 중요성 입증

#### 4.3 향후 연구 시 고려할 점

**1. 일반화와 견고성**:[12][13][5]
- 도메인 무작위화와 bisimulation 방법의 결합
- 절차적으로 생성된 환경에서의 평가 필수

**2. Sim-to-Real 격차 극복**:[4]
- 작은 규모 물리적 차량(1/10 스케일)에서의 실제 테스트
- 동역학 모델 오류 특성 분석

**3. 인간-AI 협력**:
- 규칙 기반 규범에서 **적응형 규범 학습**으로 전환
- 문맥 기반 페널티 시스템 고도화

**4. 다중 에이전트 시나리오**:
- 8-16 대 이상의 복잡한 상황
- 팀 스포츠(포뮬러 1 스타일) 시뮬레이션

**5. 설명 가능성(XAI)**:
- AI 드라이버의 의사결정 과정 시각화
- 드라이버 교육용 피드백 생성

**6. 전이 학습**:
- GT Sport → GT 7 → 다른 시뮬레이터 (CARLA 등) 전이 가능성
- 다양한 차량 클래스(F1, 트럭 등)로의 확대

#### 4.4 윤리적 고려사항

**게임 플레이어 경험**:
- AI의 불공정한 이점 제거
- 플레이어 스킬 수준별 조정 (GT Sophy 2.1에서 부분 해결)

**자율주행 적용**:
- 도로 안전 기준 충족 필수
- 투명성과 예측 가능성 확보
- 법적 책임 한계 명확화

***

### 요약

**Gran Turismo Sophy**는 심층 강화학습의 **실제 연속 제어 문제 적용**의 이정표입니다. QR-SAC 알고리즘, 혼합 시나리오 훈련, 다중 목표 정책 선택 등 혁신적 방법론을 통해 세계 최고 수준의 경기 성능을 달성했습니다.

향후 **vision 기반 입력, 온라인 정책 적응, 도메인 무작위화, sim-to-real 전이** 등의 기법 적용으로 일반화 성능을 크게 향상시킬 수 있습니다. 특히 최신 연구(2023-2025)는 **멀티모달 학습, 메타 러닝, bisimulation 기반 표현 학습** 방향으로 진화 중이며, 이들을 GT Sophy에 통합하면 자율주행, 로봇 제어, 시뮬레이션 기반 훈련 등 광범위한 실제 응용이 가능해질 것으로 예상됩니다.

***

### 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b3fd9305-ba42-4d17-a746-ce8409c879fb/s41586-021-04357-7.pdf)
[2](https://arxiv.org/html/2406.12563)
[3](https://arxiv.org/html/2407.00898v2)
[4](https://www.nature.com/articles/s41586-023-06419-4)
[5](https://journals.sagepub.com/doi/10.1177/17298806251332359)
[6](https://arxiv.org/pdf/2412.03803.pdf)
[7](http://arxiv.org/pdf/2402.14194.pdf)
[8](https://aihub.org/2022/07/27/sonys-gran-turismo-sophy-project-wins-the-acm-sigai-industry-award/)
[9](https://ai.sony/articles/Sony-AI-Expands-Capabilities-of-Breakthrough-AI-Agent-Gran-Turismo-Sophy-Offering-Players-a-More-Flexible-and-Personalized-Experience/)
[10](https://arxiv.org/abs/2103.14666v2)
[11](https://arxiv.org/pdf/2308.13491.pdf)
[12](https://neurips.cc/virtual/2023/77286)
[13](https://ezgikorkmaz.github.io/Reinforcement_Learning_Survey_NeurIPS23.pdf)
[14](https://arxiv.org/pdf/1912.10944.pdf)
[15](https://arxiv.org/pdf/2311.16339.pdf)
[16](https://www.gtplanet.net/gran-turismo-7s-sophy-artificial-intelligence-driving-system-revealed/)
[17](https://sonyinteractive.com/en/news/blog/gran-turismo-sophy/)
[18](https://www.sciencedirect.com/science/article/pii/S266682702300049X)
[19](https://ieeexplore.ieee.org/document/10328086/)
