# Trust Region Policy Optimization

**주요 결론:** Trust Region Policy Optimization(TRPO)은 정책 업데이트 시 **KL 발산**을 신뢰 영역(constraint)으로 사용하여 **모노토닉 개선(일관된 성능 향상)**을 보장하는 실용적 알고리즘을 제안한다. 이 절차는 자연 정책 기울기(natural policy gradient)와 유사하며, 대규모 비선형 정책(예: 신경망) 최적화에 효과적이다.[1]

## 1. 간결 요약
TRPO는 다음의 핵심 아이디어에 기반한다:[1]
- **문제**: 근사(policy approximation) 환경에서 정책 반복(policy iteration)이 항상 성능을 개선하지 못하는 한계를 극복하고자 함.
- **접근**: 기존의 보수적 정책 반복(conservative policy iteration) 이론을 일반 확률적 정책(stochastic policies)으로 확장하고, 이론상 보장된 개선 하한(lower bound)을 실용 가능한 형태로 근사.
- **제안**: KL 발산($$D_{\mathrm{KL}}$$)을 제약 조건으로 사용하는 신뢰 영역 업데이트  

$$
    \begin{aligned}
      &\max_\theta\; L_{\rm surrogate}(\theta) = \mathbb{E}_{s,a\sim\pi_{\rm old}}\Big[\frac{\pi_\theta(a|s)}{\pi_{\rm old}(a|s)}\,A_{\rm old}(s,a)\Big],\\
      &\text{s.t.}\quad \mathbb{E}_{s\sim \rho_{\rm old}}\big[D_{\mathrm{KL}}\big(\pi_{\rm old}(\cdot|s)\,\|\,\pi_\theta(\cdot|s)\big)\big]\le\delta
    \end{aligned}
  $$

- **성과**: 시뮬레이션 기반 로봇 보행 및 Atari 게임에서 **안정적이고 강건한** 학습 성능을 입증했으며, 하이퍼파라미터 튜닝이 거의 필요 없다.[1]
- **기여**: 정책 반복과 정책 기울기 방법을 **통합**하는 이론적 관점과, 대규모 정책 최적화에 적용 가능한 **실용적 알고리즘** 제시.

## 2. 문제 정의
무한 시간 할인된 마르코프 의사결정과정(MDP)에서, 정책 $$\pi_\theta$$의 기대 누적 보상  

$$
  J(\theta)=\mathbb{E}\Bigl[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\Bigr]
$$

를 최대화하는 것이 목표이다.  
근사 환경에서는 정책 업데이트가 일부 상태에서 성능을 저하시킬 수 있어, **보수적(iteration) 보장을 갖춘** 업데이트가 필요하다.[1]

## 3. 제안 방법
### 3.1 이론적 보장
Kakade & Langford(2002)의 보수적 정책 반복 결과를 일반 확률적 정책으로 확장하여, **KL 발산**에 기반한 개선 하한을 유도함:[1]

```math
  J(\theta_{\rm new}) \ge L_{\rm old}(\theta_{\rm new})
  - C\,\max_s D_{\mathrm{KL}}\bigl(\pi_{\rm old}(\cdot|s)\,\|\,\pi_{\rm new}(\cdot|s)\bigr),
```

여기서 $$L_{\rm old}$$는 서로게이트 목적 함수, $$C$$는 이론적 상수다.

### 3.2 실용적 근사
1. **평균 KL 제약** 도입  
   최대 KL 제약 대신  

$$\mathbb{E}\_{s\sim\rho_{\rm old}}[D_{\mathrm{KL}}]\le\delta$$  
   
   로 단순화하여 실용성 확보.[1]
2. **샘플 기반 근사**  
   - **Single-path**: 시뮬레이션 궤적에서 직접 Advantage 추정  
   - **Vine**: 특정 상태에서 다수 액션 롤아웃, Common Random Numbers 사용하여 분산 저감.[1]
3. **최적화**  
   - 목적 함수 선형화, KL 제약의 2차 근사(Fisher 정보 행렬) 이용  
   - **Conjugate Gradient**로 $$A^{-1}g$$ 근사 해 구하고, **라인 서치**로 실제 KL 제약 만족 및 개선 보장.[1]

## 4. 모델 구조
- **연속 동작**(로봇): 상태→다층 퍼셉트론→정규분포 평균, 분산은 독립 학습 파라미터  
- **이산 동작**(Atari): 상태→합성곱 신경망→소프트맥스 팩터화된 카테고리 분포  
- 수만 개 파라미터 규모의 비선형 정책을 효과적으로 최적화 가능.[1]

## 5. 성능 향상 및 한계
- **향상**:  
  - 로봇 보행 과제에서 **모든** 문제(수영, 도약, 보행) 해결  
  - Atari 게임에서 안정적 점수 달성, 최소 튜닝으로 **강건성** 입증.[1]
- **한계**:  
  - **계산 비용**: Conjugate gradient 및 시뮬레이션 롤아웃 다수 필요  
  - **샘플 복잡도**: 모델 부재 시 실제 환경 적용 시 표본 요구량 큼  
  - **추정 오차**: Advantage·KL 근사에 따른 이론·실제 차이 존재

## 6. 일반화 성능 향상 가능성
- KL 제약은 **과도한 정책 변화를 억제**하여, 과적합을 방지하고 **일관된 업데이트**를 가능하게 함.[1]
- Vine 샘플링의 분산 저감 기법은 **Advantage 추정**의 넓은 일반화를 지원.
- 제약 기반 접근이 **다양한 환경 변화**에 대한 강건성을 제공하여, **새로운 상태 분포**에서도 안정적 성능 유지 기대.

## 7. 향후 연구 영향 및 고려 사항
- **통합 학습**: 인지·제어 동시 학습 위해 **시각+로봇 제어** 정책 공동 최적화  
- **재발견 정책**: 순환 신경망 등 숨은 상태 활용, 부분관찰 환경으로 확장  
- **모델 학습 결합**: dynamics 모델과 결합해 **표본 효율성** 개선, 실제 로봇 적용  
- **이론적 확장**: 추정 오차 분석 포함한 엄밀한 수렴 보장, 이론·실제 격차 해소

TRPO는 정책 반복과 정책 기울기 방법을 **통합**하고, KL 기반 **보수적 업데이트**로 **모노토닉 개선**을 보장하여 대규모 비선형 정책을 안정적으로 학습할 수 있는 기틀을 마련했다. 이는 향후 **강건한 강화학습** 및 **현실 환경 적용**을 위한 핵심 프레임워크로 자리잡을 것이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4b61733f-ccb6-4501-a659-5607f51930b3/1502.05477v5.pdf)
