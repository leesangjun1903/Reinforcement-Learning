# Learning values across many orders of magnitude

# 핵심 요약  
**주요 주장:** 함수 값의 크기가 크게 변해도 학습 알고리즘이 안정적으로 동작하려면, 목표값(target)에 대한 **적응적 정규화(adaptive normalization)**가 필요하다. 이를 통해 도메인별 보편적 휴리스틱 없이도 안정적인 학습이 가능하다.[1]
**주요 기여:**  
- Pop-Art(Preserving Outputs Precisely, while Adaptively Rescaling Targets)라는 기법 제안.[1]
- 보상 클리핑 없이도 DQN 기반 심층강화학습에서 성능 유지·향상 입증.[1]

# 문제 정의  
깊은 강화학습(DQN 등)에서 환경마다 보상 크기와 빈도가 크게 달라지면, 학습률(step size)과 손실 스케일을 일괄적으로 설정하기 어렵다. 전통적으로 Atari 도메인에서는 보상을 ±1로 클리핑했으나, 이는 본래 목표(sum of rewards)를 변형시켜 비일관적 정책을 초래한다.[1]

# 제안 방법  
## Pop-Art 알고리즘  
1. 목표 $$Y_t$$를 평균 $$\mu_t$$, 분산 $$\sigma_t^2$$로 정규화:  

$$
     \tilde Y_t = \frac{Y_t - \mu_t}{\sigma_t}
   $$  

2. 정규화 파라미터 $$\mu_t, \sigma_t$$를 독립적 목적 함수로 학습(식 (4) 참조)하여 비정규화 함수 $$f(x)$$의 출력을 보존하도록 출력 계층 파라미터 $$W, b$$를 함께 조정.[1]
3. 정규화된 오차로 네트워크를 업데이트하여 학습 안정화.  

### 수식(식 (2), (4))  
- 비정규화 함수:  

$$
    f(x)=\sigma_t\,g(x;W,b)+\mu_t
  $$  

- 정규화 통계량 업데이트:  

$$
    \mu_t = (1-\beta)\,\mu_{t-1} + \beta\,Y_t,\quad
    \sigma_t^2 = (1-\beta)\,\sigma_{t-1}^2 + \beta\,(Y_t - \mu_t)^2
  $$  
  
# 모델 구조  
일반적인 DQN 네트워크 최종 출력 직전에 affine 계층을 두고, 이 계층의 출력에 Pop-Art 정규화를 적용한다. 정규화 전 파라미터와 정규화 파라미터는 동시에 업데이트된다.[1]

# 성능 향상  
- **이진 회귀 실험:** 희귀한 큰 값이 간헐적으로 등장해도 Pop-Art 적용 시 RMSE 감소 및 빠른 수렴 관찰.[1]
- **Atari 57게임:**  
  - 보상 클리핑 없이도 Pop-Art 적용 시, 그래디언트 노름 분포가 2개 이하의 로그 스케일 범위로 집중되어 안정적 학습 가능.[1]
  - 전체 57게임 중 32게임에서 클리핑 버전 대비 성능 유지 또는 향상(중위수 0.4, 평균 34 포인트 차이) 확인.[1]

# 한계  
- 복잡한 환경에서 정규화 파라미터 추정이 과도하게 반응할 수 있으며, 초기화 민감성 존재.[1]
- 할인율이 높지 않은 환경에서는 여전히 단기 보상 최적화에 치우칠 가능성.  

# 일반화 성능 향상 가능성  
Pop-Art는 비정규화 함수의 출력 보존을 보장하면서, 다양한 스케일의 목표값에 자동 적응하므로 다음과 같은 장점을 통해 일반화 성능을 향상시킬 수 있다.  
- **도메인 무관성:** 보상 크기에 대한 휴리스틱(클리핑) 제거로 새로운 환경에 바로 적용 가능.  
- **비정상성 대응:** 정책 변경에 따른 보상 분포 변화에도 빠르게 적응.  
- **멀티태스크:** 서로 다른 스케일의 여러 보상을 동시에 예측해야 하는 멀티태스크 학습에서 손실 구성요소 간 상호간섭 감소.  

# 향후 연구 영향 및 고려사항  
Pop-Art는 강화학습뿐 아니라 **감독학습의 온라인(setting)** 에서도 타겟 정규화 문제를 해결할 수 있는 일반적 프레임워크를 제공한다.  
향후 연구 시 다음 사항을 고려할 필요가 있다.  
- 정규화 파라미터의 **초기화 전략** 및 **학습률(β)** 설정 최적화.  
- **하위 계층(normalization depth)** 적용 위치 탐색(Deep Pop-Art).  
- **다른 최적화 기법(RMSprop, Adam)** 과의 결합 효과 분석.  
- **비정규 분포**의 타겟에 대한 퍼센타일 기반 정규화 확장 검토.  

Pop-Art는 스케일 변화에 강건하면서 본래 목표를 보존하므로, 다양한 강화학습 및 온라인 학습 분야에서 **일반화 성능 향상의 핵심 기법**으로 활용될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/37b8749e-50a3-4d09-8853-33d3be9a40e7/1602.07714v2.pdf)
