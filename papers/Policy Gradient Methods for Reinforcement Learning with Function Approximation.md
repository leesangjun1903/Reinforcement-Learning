# Policy Gradient Methods for Reinforcement Learning with Function Approximation

**핵심 주장 및 주요 기여**  
이 논문은 강화학습에서 함수 근사(Function Approximation)를 활용할 때 기존 가치 함수 기반 접근법의 이론적 한계를 극복하기 위해, 정책(Policy)을 직접 매개변수화하고 기대보상을 매개변수에 대한 그라디언트로 최적화하는 **정책 그라디언트(Policy Gradient)** 접근법을 제안한다. 주요 기여는 다음과 같다.  
- 정책 성능 지표(평균 보상 또는 시작 상태 기반 누적 보상)의 그라디언트를 샘플 경험으로부터 편향 없이 추정할 수 있는 형태로 **정책 그라디언트 정리(Policy Gradient Theorem)**를 증명.[1]
- 근사된 행동가치 함수 또는 어드밴티지 함수(Advantage Function)를 이용해 그라디언트 추정 분산을 줄이고 학습 속도를 개선하는 **액터-크리틱(Actor–Critic)** 계열 알고리즘의 수렴성을 최초로 보장.[1]
- 정책과 가치 함수 근사의 **호환성 조건(compatibility condition)**을 도입하여, 근사 가치 함수가 정책 근사와 동일한 특징(feature) 구조를 가져야 함을 제안.[1]
- 위 이론에 기반해 매개변수화된 정책 반복(Policy Iteration) 과정에서도 국소 최적성(local optimum)으로 수렴함을 엄밀히 증명.[1]

***

## 1. 해결하고자 하는 문제  
기존 강화학습 함수 근사 방식은 가치 함수 $$V(s)$$ 또는 $$Q(s,a)$$를 근사하고, 이를 기반으로 암묵적(implicit)인 탐욕 정책(greedy policy)을 유도한다.  
이 방식의 주요 한계는  
- 최적 정책이 흔히 확률적(stochastic)인 반면 가치 함수 접근은 결정적(deterministic) 정책에 치우침.  
- 가치 함수의 작은 오차가 탐욕 정책에서 비연속적(discontinuous)인 큰 정책 변경을 초래해 수렴 보장이 어려움.[1]

이를 해결하기 위해, 논문은 정책 $$\pi(a|s;\theta)$$를 직접 매개변수화하고 성능 척도 $$J(\theta)$$에 대한 그라디언트 $$\nabla_\theta J(\theta)$$를 직접 추정하여 매개변수를 갱신하는 방법을 제안한다.

***

## 2. 제안 방법  
### 2.1 성능 그라디언트 정리 (Policy Gradient Theorem)  
평균 보상(formulation 1) 혹은 시작 상태 기반 누적 보상(formulation 2) 모두에 대해, 성능 지표 $$J(\theta)$$의 그라디언트는 다음과 같이 표현된다:[1]

$$
\nabla_\theta J(\theta)
= \sum_{s} d^{\pi}_\theta(s) \sum_{a}
\nabla_\theta \pi(a|s;\theta)\,Q^\pi(s,a)
$$

여기서 $$d^{\pi}_\theta(s)$$는 정책 $$\pi$$에 따른 상태 분포, $$Q^\pi(s,a)$$는 행동가치 함수이다.

### 2.2 어드밴티지 함수 근사 & 호환성 조건  
행동가치 함수의 직접 사용은 분산이 크므로, 논문은 **어드밴티지 함수**  

$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$

를 근사할 것을 권장하며, 근사 함수 $$f_w(s,a)$$는 다음 호환성 조건을 만족해야 한다:[1]

$$
\nabla_w f_w(s,a)\;=\;\nabla_\theta \log \pi(a|s;\theta)
$$

즉, $$f_w$$는 정책의 특징(feature)과 동일한 선형 구조를 가져야 하며, 상태별 평균이 0이 되도록 정규화된 형태이다.

### 2.3 학습 알고리즘  
1. **액터 업데이트(Actor)**:  

$$\theta_{t+1} = \theta_t + \alpha_t \, f_w(s_t,a_t)\,\nabla_\theta\log\pi(a_t|s_t;\theta_t)$$  

2. **크리틱 업데이트(Critic)**:  

$$w_{t+1} = w_t + \beta_t \,\delta_t\,\nabla_w f_w(s_t,a_t)$$,  
   
$$\delta_t = r_t + \gamma\,f_w(s_{t+1},a_{t+1}) - f_w(s_t,a_t)$$  

이때 학습률 $$\alpha_t,\beta_t$$는 수렴 조건을 만족해야 하며, 액터-크리틱 구조가 국소 최적 해로 수렴함을 보인다.[1]

***

## 3. 모델 구조  
- **정책 네트워크**: 상태 $$s$$를 입력으로 받아 각 행동 $$a$$에 대한 확률 $$\pi(a|s;\theta)$$을 출력하는 함수 근사기(예: 신경망).  
- **어드밴티지 네트워크**: 정책과 동일한 특징을 공유하는 선형 근사기로, 상태-행동 쌍 $$(s,a)$$를 입력해 $$A^\pi(s,a)$$를 출력.  
- **탐험–수렴 균형**: 확률적 정책 및 어드밴티지 근사는 정책 변화가 부드러워서 탐험(exploration)과 수렴(convergence)을 동시에 보장.

***

## 4. 성능 향상 및 한계  
- **분산 감소**: 가치 함수 근사를 이용해 REINFORCE보다 그라디언트 추정 분산을 크게 줄여 **학습 속도** 및 **표본 효율성** 개선.[1]
- **이론적 수렴 보장**: 임의의 미분 가능 함수 근사기에서도 정책 반복(Policy Iteration)이 국소 최적성에 수렴함을 최초로 증명.[1]
- **한계**:  
  - 전역 최적(global optimum)이 아닌 **국소 최적(local optimum)** 에 머무를 수 있음.  
  - 근사 구조 선택 및 하이퍼파라미터(학습률 등)에 민감함.  
  - 높은 차원의 상태·행동 공간에서 여전히 표본 복잡도(sample complexity)가 클 수 있음.

***

## 5. 일반화 성능 향상 가능성  
- **어드밴티지 베이스라인 활용**: 상태별 임의 함수 $$v(s)$$를 베이스라인으로 추가해 분산을 더욱 줄일 수 있으며, 근사 모델이 주요 특징에 집중하도록 유도한다.[1]
- **특징 설계 및 공유**: 정책과 어드밴티지 근사기 간 특징 공유는 모델의 파라미터 수를 줄이고 과적합(overfitting)을 방지함으로써 일반화 성능을 향상시킨다.  
- **정책 구조 확장**: 비선형 정책 네트워크(예: 다층 퍼셉트론, 컨볼루션 신경망, 트랜스포머)와 호환성 조건을 준수하도록 어드밴티지 근사기를 설계하면 복잡한 환경에서도 일반화가 가능하다.

***

## 6. 향후 연구에 미치는 영향 및 고려사항  
이 논문은 강화학습에서 **직접 정책 최적화**라는 연구 분야를 견고히 하였으며, 심층 강화학습(Deep RL), 멀티에이전트 RL, 메타 RL 등 다양한 후속 연구에 핵심 이론적 기틀을 제공하였다.  
향후 연구에서는  
- **전역 최적화 기법**(예: 자연 그라디언트, 신뢰 영역 최적화)을 도입해 국소 최적 문제를 완화할 것.  
- **모델 기반 RL**과 결합하여 표본 효율성을 더욱 개선.  
- **대규모 비선형 근사기**(예: 심층 신경망)와 안정적 학습 기법(배치 정규화, 잔차 학습)을 적용해 복잡한 실제 환경에서 일반화 성능을 극대화할 것.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/be3b37e5-d5a3-4a34-9d9c-792f58bb5a32/NIPS-1999-policy-gradient-methods-for-reinforcement-learning-with-function-approximation-Paper.pdf)
