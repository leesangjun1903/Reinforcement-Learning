# AlphaGo : Mastering the Game of Go without Human Knowledge 

**핵심 주장 및 주요 기여**  
“Mastering the Game of Go without Human Knowledge” 논문은 **인간 데이터 전혀 사용 없이**, 오로지 **자의적(self-play) 강화학습**만으로 바둑에서 초인적(superhuman) 기량을 달성할 수 있음을 증명한다. 주요 기여는 다음과 같다.  
- **탐색과 학습의 통합**: 단일 신경망 $$f_\theta(s)=(p,v)$$을 이용해 정책(policy)과 가치(value)를 동시에 예측하며, MCTS를 정책 개선(policy improvement) 및 평가(policy evaluation)에 직접 통합.  
- **무(無)인간 데이터 학습**: 전문가 기보나 휴리스틱 없이 완전 무(無)지식 상태에서 시작(tabula rasa)해 3일 만에 AlphaGo Lee를 100:0으로 압도.  
- **간결한 아키텍처**: 20~40개의 잔차(residual) 블록을 쌓은 컨벌루션 네트워크 한 개만 사용, 별도 rollout 없음.  
- **안정적 수렴**: 학습 초기의 불안정성 없이 꾸준한 성능 향상 그래프(Elo rating) 관찰.  

***

## 1. 문제 정의  
기존 AlphaGo 계열은 **인간 기보 데이터**를 기반으로 한 **지도학습**과 **rollout 기반 MCTS**를 결합해 성능을 높였으나,  
1) **전문가 데이터 의존성**  
2) **수작업 특징 설계**  
3) **학습 성능 한계**  
를 안고 있다.  
이 논문은 “바둑 규칙만 알면”, 인간 지식 없이도 순수 강화학습만으로 최적 정책을 학습할 수 있는 **범용 학습 프레임워크**를 제안한다.  

***

## 2. 제안 방법  
### 2.1 네트워크 및 MCTS 통합  
- 단일 신경망 $$f_\theta(s)$$가 입력 상태 $$s$$에 대해  

$$
    f_\theta(s) = (p, v),\quad p_a = \Pr(a\mid s),\quad v \approx \mathbb{E}[\text{win}\mid s]
  $$  
  
  두 출력을 모두 생성.  
- MCTS 시뮬레이션에서 leaf 노드 확장 시 단 한번 $$f_\theta$$로 $$(P(s,\cdot), V(s))$$를 계산하고,  
  $$Q$$-값과 방문 횟수 $$N$$를 백업.  
- 탐색 후 얻은 검색 확률 $$\pi$$와 실제 승패 $$z$$를 학습 신호로 사용.  

### 2.2 학습 목표식  
신경망 매개변수 $$\theta$$는 다음 손실함수에 따라 갱신된다.  

$$
\ell(\theta) = (z - v)^2 - \boldsymbol{\pi}^\mathsf{T}\!\log \boldsymbol{p} + c\|\theta\|^2,
$$  

여기서  
- $$(z-v)^2$$: 가치 예측 MSE  
- $$-\pi^T\log p$$: 정책 교차엔트로피  
- $$c\|\theta\|^2$$: L2 정규화  

### 2.3 학습 파이프라인  
1. **Self-play 생성**: 최고 성능 네트워크로 25,000게임 자가 대국, 초기 30수는 온도 $$\tau=1$$, 이후 $$\tau\to0$$.  
2. **네트워크 업데이트**: 최근 500,000게임에서 추출한 위치와 $$(\pi,z)$$ 샘플로 SGD.  
3. **모델 선발(evaluator)**: 새 체크포인트를 400게임 평가, 승률 >55% 시 갱신.  

***

## 3. 모델 구조  
- **입력**: $$19\times19$$ 바둑판 × 17개의 feature plane (현재·과거 8수씩 흑·백, 현재 수순)  
- **잔차 블록**: 컨벌루션(3×3, 256채널) + 배치정규화 + ReLU × (20 or 40)개 반복  
- **정책 헤드**: $$1\times1$$ 컨벌루션 → FC(362) → 소프트맥스  
- **가치 헤드**: $$1\times1$$ 컨벌루션 → FC(256) → ReLU → FC(1) → tanh  

***

## 4. 성능 향상  
- **학습 곡선**: 36시간 만에 AlphaGo Lee 능가, 72시간 후 100:0 승리.  
- **아키텍처 비교**:  
  | 아키텍처 | Elo (5s) | 예측 정확도 | MSE값 |
  |:--------:|:--------:|:-----------:|:-----:|
  | sep-conv (AlphaGo Lee) | … | … | … |
  | dual-res (본 논문)     | +1200 Elo↑ | 정확도 ↓소폭 | MSE ↓ |  
  *Residual + 통합 헤드가 가장 우수*  
- **대규모 학습(40일)**: 29M self-play, 40블록 네트워크로 최종 Elo 5185 도달.  

***

## 5. 한계 및 일반화 성능  
- **탐색 의존도**: MCTS 시뮬레이션 횟수가 많아야 높은 성능  
- **계산 자원**: TPU 클러스터 필수, 학습 비용 높음  
- **일반화 가능성**:  
  - **규칙만 알면 다른 완전정보 제로섬 게임에도 적용** 가능  
  - 네트워크 입력·출력만 바꾸면 **체스·쇼기** 등으로 확장  
  - 순수 강화학습으로 **도메인지식 없이** 일반화 성능 개선이 기대됨  

***

## 6. 향후 연구 영향 및 고려 사항  
- **영향**:  
  - AI 학습에 필요한 인간 데이터 의존을 획기적으로 감소  
  - 순수 강화학습 알고리즘이 다양한 전략게임·시뮬레이션에 적용 가능성 제시  
- **고려 사항**:  
  - **효율적 학습**을 위해 탐색-모델 결합 최적화  
  - **자원 제약 하**에서의 경량화 및 샘플 효율성 개선  
  - **불완전정보 게임**으로의 확장 및 멀티에이전트 환경 적용 연구  

이로써 인간 지식 없이도 자가 학습만으로 초인적 기량을 달성할 수 있음을 입증하며, AI 학습 패러다임 전환의 기반을 마련하였다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/956fee78-bb77-4303-8811-535b803e18c8/nature24270.pdf)
