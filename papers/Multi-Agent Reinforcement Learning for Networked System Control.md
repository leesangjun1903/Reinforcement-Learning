# Multi-Agent Reinforcement Learning for Networked System Control

### 1. 논문의 핵심 주장과 주요 기여

본 논문은 네트워크 시스템 제어를 위한 멀티에이전트 강화학습(MARL) 문제를 체계적으로 정식화하고, 실제 제어 시스템의 제약을 반영한 두 가지 핵심 기여를 제시합니다.[1]

**핵심 주장:**
논문의 중심 주장은 기존 MARL 알고리즘이 게이밍 환경의 가정(전역 관찰, 온라인 학습)에 기반하고 있어, 실제 네트워크 제어 시스템(교통신호제어, 연결된 차량 제어)에 적합하지 않다는 점입니다. 따라서 국소 관찰과 이웃 통신으로 제한된 오프라인 학습 환경에서 작동하는 알고리즘이 필요합니다.[1]

**주요 기여:**
첫째, **시공간 할인계수(Spatial Discount Factor, γ)**를 도입하여 거리가 먼 에이전트의 보상 신호를 감소시키고, 부분 관찰성과 비정상성 문제를 완화합니다. 이는 특히 비통신 알고리즘의 학습 안정성을 향상시킵니다.[1]

둘째, **NeurComm(Neural Communication Protocol)**이라 명명한 새로운 신경 통신 프로토콜을 제안합니다. 이는 기존 통신 프로토콜의 정보 손실 문제를 해결하고, 정책 지문(Policy Fingerprint)을 포함하여 비정상성을 감소시킵니다.[1]

***

### 2. 문제 정의, 제안 방법, 모델 구조, 성능 분석

#### 2.1 문제 정의: 시공간 MDP

논문은 네트워크 시스템을 그래프 $$G = (V, E) $$로 표현하며, 각 에이전트 $$i \in V $$는 이웃 $$N_i $$에서만 정보를 수신합니다. 표준 다중 에이전트 MDP를 기반으로 **시공간 MDP(Spatiotemporal MDP)**를 정의합니다:[1]

**정의 3.2 (시공간 MDP)**
국소 전이가 이웃 에이전트에만 의존한다는 가정 하에:

$$p_i(s_{i,t+1} | s_{V_i,t}, a_{i,t}, a_{N_i,t}) = p(s_{i,t+1} | s_{V_i,t}, a_{i,t}, a_{N_i,t}) $$

여기서 $$V_i = N_i \cup \{i\} $$는 폐쇄 이웃입니다.[1]

시공간 MDP의 목표는 **시공간 할인된 수익(Spatiotemporal Discounted Return)**을 최대화하는 것입니다:

$$R_{i,t}^{\gamma} = \sum_{t'=t}^{T} \gamma^{t'-t} \sum_{j \in V} d_{ij}^{\gamma} r_{j,t'} $$

여기서 $$\gamma $$는 시공간 할인계수, $$d_{ij} $$는 에이전트 i와 j 사이의 거리입니다. 이 공식은 지리적으로 먼 에이전트의 보상 신호를 감소시키므로, 에이전트는 자신의 국소 정보만으로 더 쉽게 학습할 수 있습니다.[1]

#### 2.2 제안 방법: 시공간 RL과 NeurComm

**A2C 기반 학습 알고리즘**

논문은 A2C(Advantage Actor-Critic) 프레임워크를 시공간 MDP에 적용합니다. 각 에이전트 i의 손실함수는:[1]

$$L_i = -\mathbb{E}[\log \pi_i(a_i | s_i) A_i + \beta H(\pi_i)] + \mathbb{E}[\frac{1}{2}(R_i - V_i(s_i, a_{N_i}))^2] $$

여기서:
- $$A_i = R_i - V_i $$는 어드밴티지 함수
- $$H(\pi_i) $$는 정책의 엔트로피(탐험 유도)
- $$R_i = \sum_{t'=t}^{T} \gamma^{t'-t} \sum_{j \in V} d_{ij}^{\gamma} r_{j,t'} $$는 시공간 할인된 수익

이 수식은 표준 A2C를 시공간 설정으로 확장한 것입니다.[1]

**NeurComm: 신경 통신 프로토콜**

NeurComm의 핵심은 신뢰할 수 있는 메시지 인코딩과 상태 및 정책 정보의 명시적 포함입니다:[1]

$$h_{i,t} = g_i(h_{i,t-1}, e_s(s_{V_i,t}), e_p(\pi_{N_i,t-1}), e_h(h_{N_i,t-1})) $$

여기서:[1]
- $$h_{i,t} $$: 에이전트 i의 숨겨진 상태(신념)
- $$e_s, e_p, e_h $$: 상태, 정책, 숨겨진 상태를 인코딩하는 미분 가능 함수
- $$g_i $$: 메시지를 추출하는 함수(LSTM 사용)

메시지는 명시적으로 상태와 정책 지문을 포함합니다:[1]

$$m_{i,t} = \{s_{i,t}, \pi_{i,t-1}, h_{i,t-1}\} $$

**기존 프로토콜과의 차이점**

CommNet과 DIAL은 수신된 메시지를 합산하거나 평균하여 정보 손실을 초래합니다. NeurComm은 메시지를 연결(concatenation)하여 정보 손실을 최소화하고, 정책 지문을 포함하여 비정상성을 감소시킵니다.[1]

#### 2.3 모델 구조

**신경망 아키텍처**
- 메시지 인코딩층: 완전연결층(FC)으로 다양한 입력 신호 타입에 대한 암시적 정규화 학습
- 메시지 추출층: LSTM(64개 유닛) 사용
- 정책 네트워크: $$\pi_i(h_{i,t}) $$
- 가치 함수: $$V_i(h_{i,t}, a_{N_i,t}) $$

**메시지 크기**
- NeurComm, DIAL: $$O(|s_i| + |h_i|) $$
- CommNet: $$O(|s_i| + |h_i|) $$  
- FPrint: $$O(|\pi_i|) $$
- IA2C, ConseNet: 0(통신 없음)[1]

#### 2.4 성능 향상

**적응형 교통신호제어(ATSC)**

ATSC 그리드 시나리오에서 NeurComm은 다른 모든 알고리즘을 능가합니다:[1]

| 메트릭 | NeurComm | CommNet | DIAL | IA2C | FPrint |
|--------|----------|---------|------|------|--------|
| 평균 큐 길이(veh) | **1.16** | 1.44 | 2.36 | 1.63 | 1.62 |
| 평균 교차로 지연(s/veh) | **68** | 111 | 145 | 376 | 415 |

ATSC 모나코(현실 네트워크)에서도 우수한 성능을 보이며, 특히 비통신 알고리즘(IA2C, FPrint)의 성능 저하가 큽니다.[1]

**협력형 적응 순항제어(CACC)**

CACC 캐치업 시나리오에서 FPrint가 최고 실행 성능(-57.8)을 보이지만, NeurComm(-94.6)은 CommNet과 비슷하며 경쟁력을 유지합니다. 이는 실시간이고 안전이 중요한 작업에서는 지연된 정보 공유가 도움이 될 수 있음을 시사합니다.[1]

#### 2.5 한계

**학습 수렴의 한계**
- CACC 작업에서 진동 제거 어려움: 플래툰의 첫 번째 차량은 목표값 안정화, 마지막 차량은 여전히 진동(그림 6)[1]
- 이는 휴리스틱 저수준 제어기와 지연된 정보 공유의 결합 효과로 추정됨[1]

**일반화 문제**
- ATSC 모나코에서 통신 알고리즘의 과적합 위험: 짧은 소스 엣지의 큐를 의도적으로 증가시켜 도로상 차량 감소(보상 감소)[1]
- 이는 동질의 네트워크(그리드)에서는 캡처하지 못하는 현실적 이질 네트워크의 복잡성을 보여줍니다[1]

***

### 3. 모델의 일반화 성능 향상 가능성

#### 3.1 현재 논문의 일반화 성능 분석

**시공간 할인계수의 역할**

표 1에서 보면, 최적의 γ 값이 작업 난이도에 따라 달라집니다:[1]
- 쉬운 작업(CACC Catch-up): γ = 1.0 (모든 알고리즘)
- 어려운 작업(ATSC Monaco): γ = 0.9 (대부분 비통신 알고리즘)
- 매우 어려운 작업(CACC Slow-down): γ = 0.8 (IA2C)

이는 γ가 **문제 난이도와 에이전트 조율 수준의 정보 지표**로 작동함을 의미합니다. 적절한 γ 선택은 각 에이전트가 국소 정보만으로 전역 문제를 해결할 수 있도록 합니다.[1]

**NeurComm의 일반화 효과**

그림 3의 절제 연구에서 NeurComm의 구성 요소가 다양한 작업에서 성능을 향상시킵니다:[1]
- **Concat Only**: 상태와 정책 정보를 연결하여 기본 프로토콜 개선
- **FPrint Only**: 정책 지문만으로도 비정상성 감소
- **NeurComm**: 두 효과가 누적되어 가장 큰 성능 향상

이는 서로 다른 작업 특성에 대한 NeurComm의 견고한 적응성을 보여줍니다.[1]

#### 3.2 최신 연구 기반 일반화 향상 방향

**1. 데이터 증강을 통한 일반화(2024)**

최근 연구에 따르면, MARL 에이전트의 일반화 능력은 데이터 증강으로 강화될 수 있습니다. GMARL(Generalized Multi-Agent Reinforcement Learning) 프레임워크는 차등 프라이버시 기반 데이터 증강을 통해 경쟁 정책의 일반화 능력을 개선합니다.[2]

논문의 NeurComm과 결합하면, 학습 중에 다양한 에이전트 구성(에이전트 수, 네트워크 토폴로지)에 대한 정책을 증강하여 학습하면 미세한 네트워크 변화에 더욱 견고해질 수 있습니다.[1]

**2. 의도 공유(Intention Sharing) 프로토콜(2024)**

기존 상태 공유 방식 대신 에이전트의 미래 의도(상상된 궤적)를 공유하는 새로운 통신 패러다임이 제시되었습니다. 이는 NeurComm의 현재 상태 및 정책 공유 방식을 보완하여, 다양한 에이전트 동작에 더 효과적으로 대응할 수 있게 합니다.[1]

**3. 마스크된 자동인코더를 통한 태스크 독립적 기술 학습(2025)**

MA2RL(Masked Autoencoders for Generalizable Multi-Agent RL)은 각 에이전트가 작업 독립적인 기술을 학습하여 다양한 작업과 에이전트 구성에 걸쳐 일반화하도록 합니다. 이를 논문의 시공간 MDP와 결합하면, 다양한 네트워크 토폴로지와 제어 목표에 대한 견고한 정책을 학습할 수 있습니다.[2]

**4. 멀티 라운드 통신 프레임워크(2025)**

최근 연구는 단일 라운드가 아닌 다중 라운드 통신을 지원하는 통합 MARL 프레임워크를 제안합니다. 이는 논문의 단일 패스 NeurComm을 다음과 같이 확장할 수 있습니다:[3]

$$ h_i^{(k)} = g_i^{(k)}(h_i^{(k-1)}, e_s(s_{V_i,t}), e_p(\pi_{N_i,t-1}), e_h(h_{N_i}^{(k-1)})) $$

여기서 $$k $$는 통신 라운드입니다. 다중 라운드 통신은 정보 전파 속도를 높이고, 특히 큰 네트워크에서 일반화 성능을 향상시킵니다.[3]

**5. 모델 기반 분산 정책 최적화(2024)**

최근 Nature에 발표된 연구는 국소 모델 학습으로 에이전트 수가 수백 개까지 확장되는 시스템에서도 일반화 성능을 유지합니다. 논문의 시공간 MDP는 이러한 모델 기반 접근과 결합하여 다음 상태를 예측함으로써:[4]

$$ \hat{s}_{i,t+1} = f_i(s_{V_i,t}, a_i,t, a_{N_i,t}) $$

더욱 견고하고 샘플 효율적인 학습이 가능합니다.

#### 3.3 논문 특정 일반화 한계와 개선 방안

**한계 1: 시공간 할인계수의 수동 튜닝**

논문에서 γ는 작업마다 수동으로 선택되어야 합니다. 현재 접근은 제한적입니다:
- ATSC Grid: γ = 0.9~1.0
- ATSC Monaco: γ = 0.9
- CACC Catch-up: γ = 1.0
- CACC Slow-down: γ = 0.8

**개선 방안**: 메타 학습(Meta-Learning) 기법으로 학습 중에 적응적으로 γ를 조정하는 메커니즘을 도입합니다. 최근 Hybrid Training(2024) 연구는 다중 작업에 대해 일반화된 기술을 학습하는 방법을 제시하므로, 이를 활용하여 문제별 최적 γ를 자동으로 결정할 수 있습니다.[5]

**한계 2: 이질적 네트워크에서의 과적합**

ATSC Monaco 결과에서 통신 알고리즘이 과적합되어 큐 감소를 우선시하면서 교차로 지연이 증가합니다. 이는 네트워크 토폴로지 변화에 대한 일반화 실패를 나타냅니다.[1]

**개선 방안**:[6][5]
1. **도메인 무작위화(Domain Randomization)**: 학습 중에 네트워크 토폴로지를 무작위로 변화시켜 다양한 구조에 견고하게 만듭니다.
2. **정책 증류(Policy Distillation)**: 다양한 네트워크에서 학습한 정책들을 하나의 신경망으로 통합하여 일반화 성능을 향상시킵니다.
3. **관계형 그래프 신경망(Relational GNN)**: 최근 연구에서 제안된 관계형 계획 기법과 RL의 결합은 네트워크 구조의 변화에 더 효과적으로 대응합니다.[6]

**한계 3: 안전성과 진동 제거**

CACC Slow-down에서 플래툰의 마지막 차량이 진동을 제거하지 못합니다. 이는 비정상적인 다중 에이전트 환경에서의 수렴 문제입니다.[1]

**개선 방안**:
1. **세이프 RL 프레임워크**: 확률적 보장으로 충돌을 방지하면서 학습합니다.[7]
2. **계층적 학습**: 저수준(충돌 회피)과 고수준(성능 최적화) 정책을 분리하여 학습합니다.

***

### 4. 향후 연구에의 영향과 고려사항

#### 4.1 이 논문이 앞으로의 연구에 미치는 영향

**1. 시공간 MDP 패러다임의 확산**

논문의 시공간 MDP는 네트워크 시스템 제어에서 새로운 표준으로 자리 잡을 가능성이 높습니다. 이는 국소 관찰과 이웃 통신이라는 실제 제약을 공식화하여, 이론과 실제의 괴리를 줄였습니다.[1]

**영향**:
- 스마트 그리드, 자율주행, 무선 센서 네트워크 등 다양한 응용 분야에서 시공간 MDP 기반 알고리즘 개발 촉진
- 분산 제어 시스템의 이론적 기초 강화

**2. 신경 통신 프로토콜의 진화**

NeurComm의 설계 철학(정보 손실 최소화, 정책 정보 포함)은 이후 통신 프로토콜 연구의 기준이 됩니다. 최근 연구들이 의도 공유, 다중 라운드 통신 등으로 확장한 것이 그 증거입니다.[3][1]

**3. 안전하고 실용적인 MARL의 필요성 부각**

논문의 오프라인 학습 가정과 실제 제약 반영은 학계에서 MARL의 현실화를 촉구했습니다. 최근 여러 연구가 안전한 MARL, 통신 효율, 대규모 확장성에 주목한 것이 이를 보여줍니다.[8][4][1]

#### 4.2 향후 연구 시 고려할 점

**1. 통신 효율의 정량화 및 최적화**

최근 연구(2025)에서 제시된 세 가지 통신 효율 메트릭(IEI, SEI, TEI)을 논문의 프레임워크에 통합해야 합니다:[3]
- **Information Efficiency Index (IEI)**: 단위 메시지당 성능 향상
- **System Efficiency Index (SEI)**: 시스템 전체 통신 대역폭 대비 성능 향상  
- **Task Efficiency Index (TEI)**: 작업 완료까지의 총 통신 비용

$$ \text{Task Efficiency} = \frac{\text{Final Performance}}{\sum_{\text{messages}} |m_{ij,t}|} $$

**고려사항**: 5G, IoT 등 제한된 통신 환경에서는 단순히 성능이 아닌 통신 비용 대비 성능을 최적화해야 합니다.

**2. 이질적(Heterogeneous) 네트워크 구조 처리**

ATSC Monaco의 실패 원인(이질적 네트워크에서의 과적합)을 해결하려면:[1]

$$ \text{Network Diversity} = \sum_{i} \left| \text{degree}_i - \bar{\text{degree}} \right| / |V| $$

를 학습 중에 추적하고, 해당 다양성에 대해 강건하도록 정책을 학습해야 합니다. 관계형 그래프 신경망(Relational GNN)이 좋은 후보입니다.[6]

**3. 적응형 공간 할인계수 학습**

현재 γ는 수동 튜닝이지만, 메타 학습으로 자동 적응하도록 개선합니다:[1]

$$ \gamma_i^* = \arg\max_{\gamma_i} \mathbb{E}[R_i(\gamma_i)] $$

를 학습 중에 온라인으로 최적화합니다.

**4. 대규모 네트워크 확장성 검증**

논문의 ATSC Grid(25개 교차로)와 CACC(8개 차량)는 비교적 작은 규모입니다. 최근 연구는 수백 개 에이전트까지 확장 가능함을 보이므로, 논문의 방법도 다음을 검증해야 합니다:[4]

$$ \text{Scalability Factor} = \frac{\text{Performance}(N=100)}{\text{Performance}(N=25)} $$

**5. 안전 보장과 수렴성 증명**

현재 논문은 실증적 결과만 제시합니다. 향후 연구는 다음을 증명해야 합니다:[1]
- 시공간 MDP 하에서 정책 수렴 조건
- 통신 오류나 지연 조건 하에서의 견고성
- 최악의 경우 성능 보장(Worst-case Performance Guarantee)

**6. 마이크로 모빌리티와 새로운 응용**

논문이 교통과 차량 제어에 초점을 맞춘 반면, 향후 연구는 다음으로 확장할 수 있습니다:[8]
- **공유 마이크로 모빌리티**: 전동 스쿠터, 자전거 공유 시스템의 동적 재배치
- **스마트 빌딩**: HVAC, 조명, 보안 시스템의 분산 제어
- **산업용 사물인터넷(Industrial IoT)**: 제조 시스템의 협력적 로봇 제어

***

## 결론

논문 "Multi-Agent Reinforcement Learning for Networked System Control"은 실제 네트워크 시스템의 제약을 반영한 시공간 MDP 정식화와 효율적인 NeurComm 통신 프로토콜을 제시하여, MARL 연구에서 이론과 실제의 간극을 좁혔습니다.[1]

**핵심 성과**:
- 시공간 할인계수를 통해 부분 관찰과 비정상성 문제를 효과적으로 처리
- 정보 손실 최소화와 정책 정보 포함을 통한 차별화된 통신 프로토콜 개발
- 현실적 교통 제어 및 차량 제어 환경에서 우수한 성능 입증

**향후 방향**:
최근 MARL 연구의 트렌드는 일반화 능력, 통신 효율, 대규모 확장성, 안전성 강화에 집중하고 있습니다. 논문의 프레임워크와 이들을 결합하면, 자율주행, 스마트 에너지, IoT 등 다양한 실제 응용에서 더욱 견고하고 효율적인 분산 제어 시스템을 구축할 수 있을 것으로 기대됩니다.[5][2][4][3]

***

**주요 참고문헌:**

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/1b60c045-0a5b-41bd-b58a-cfc835f891ef/2004.01339v2.pdf)
[2](https://arxiv.org/html/2502.17046v1)
[3](https://arxiv.org/pdf/2306.17052.pdf)
[4](https://arxiv.org/html/2502.14496v1)
[5](https://arxiv.org/pdf/2210.09646.pdf)
[6](http://arxiv.org/pdf/2408.13567.pdf)
[7](https://arxiv.org/pdf/2405.11106.pdf)
[8](https://arxiv.org/html/2502.19297v1)
[9](https://arxiv.org/html/2412.21088v1)
[10](https://www.sciencedirect.com/science/article/abs/pii/S0957417423022625)
[11](https://arxiv.org/html/2511.09171v1)
[12](https://www.nature.com/articles/s42256-024-00879-7)
[13](https://aclanthology.org/2025.acl-long.1459.pdf)
[14](https://openreview.net/pdf?id=qpsl2dR9twy)
[15](https://arxiv.org/html/2501.05323v1)
[16](https://arxiv.org/pdf/2507.06278.pdf)
[17](https://www.ijcai.org/proceedings/2023/0015.pdf)
[18](https://www.sciencedirect.com/science/article/pii/S1367578825000021)
[19](https://www.frontiersin.org/journals/industrial-engineering/articles/10.3389/fieng.2025.1611512/full)
