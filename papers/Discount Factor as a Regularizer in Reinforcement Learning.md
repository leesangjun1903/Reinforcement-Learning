# Discount Factor as a Regularizer in Reinforcement Learning

**핵심 주장 및 주요 기여**  
“Discount Factor as a Regularizer in Reinforcement Learning” 논문은 강화학습(RL)에서 할인율(γ)을 인위적으로 낮추어 사용하는 것이 일종의 *활성화 정규화*(activation regularization)*와 동등하며, 이는 데이터가 제한적일 때 모델의 일반화 성능을 높이는 효과적인 방법임을 보였다. 주요 기여는 다음과 같다.  
1. **이론적 동등성 증명**: TD(0), SARSA, m-step TD, LSTD 등 여러 Temporal-Difference 학습법에서 γ를 낮게 설정하는 것이 값 함수 출력의 평균 제곱값에 비례하는 정규화 항을 추가하는 것과 수학적으로 정확히 일치함을 증명.  
2. **정규화 인자 해석**: 동등성으로부터 λ = (γ_e – γ)/(2γ) 형태의 정규화 계수가 도출되며, 이는 관측된 상태 분포에 따라 값 함수 추정치의 크기를 억제함.  
3. **탐색적 실험**:  
   - *탭률 설정(GridWorld)*: 데이터 수, 상태 방문 분포의 균일도(total variation), 마르코프 체인의 mixing time에 따라 γ 조정이 일반화 성능에 미치는 영향을 체계적으로 실험.  
   - *심층 제어(MuJoCo & TD3)*: 제한된 학습 스텝 하에서 γ를 낮춰 적용했을 때, L2 정규화 대비 또는 병행 시 시뮬레이션 성능이 크게 개선됨을 다수 환경에서 확인.  

***

## 1. 문제 정의  
- **일반화(generalization)**: 제한된 수의 샘플만으로 학습한 이후, 미관측 환경에서도 안정적으로 동작하는 RL 에이전트 구현이 핵심 과제  
- **기존 접근법 한계**:  
  -  정책 엔트로피 정규화(policy-space)  
  -  값 함수 가중치에 대한 L2/L1 정규화  
  -  γ를 낮게 설정해 ‘단기 보상’에 치중(간접 정규화)  

논문은 *왜* γ를 낮추는 것이 정규화 효과를 가지며, *언제* 특히 효과적인지 명확히 설명하고자 한다.

***

## 2. 제안 방법 및 수식  
### 2.1. TD(0)에서의 등가 정규화  
일반 배치 TD(0) 업데이트(Guidance discount γ)  

$$
θ_{i+1} = θ_i + α_i\bigl(r + γ\,\hat V_{θ_i}(s') - \hat V_{θ_i}(s)\bigr)\nabla θ_i
$$  

γ<γ_e인 경우가 γ_e에 L2형 활성화 정규화 Ψ(s,θ)=λ·(ĤV_θ(s))² 추가된 형태와 동등함을 증명:  

$$
λ = \frac{γ_e - γ}{2γ},
\quad
ξ=\frac{γ_e}{γ},
\quad
α'_i = \frac{γ}{γ_e} α_i
$$  

즉, 낮은 γ로 학습하는 것은 다음 목적함수 최소화와 같다:  

$$
\mathbb{E}\_{(s,a,r,s')\sim D}\Bigl[(ξr + γ_e \hat V(s') - \hat V(s))^2\Bigr]
+λ\,\mathbb{E}_{s\sim D}\bigl[\hat V(s)^2\bigr].
$$

### 2.2. 확장 모델  
- **SARSA / Expected SARSA**: Q값 함수 추정 시에도 동일한 형태의 λ·Q² 정규화  
- **m-step TD, LSTD**: 유사한 증명으로 γ 조정이 activation regularization 항 추가와 등가

***

## 3. 모델 구조 및 실험 설정  
- **탭률 환경(GridWorld, 4×4)**: 균일/비균일 상태 방문, mixing time 제어 실험  
- **Deep RL**  
  -  알고리즘: TD3 (Twin Delayed DDPG)  
  -  환경: MuJoCo (HalfCheetah-v2, Ant-v2, Hopper-v2)  
  -  제한된 샘플(총 시뮬레이션 스텝 2e5 이하)  
  -  γ∈{0.1,…,0.99,…,1.0} 탐색 vs. critic L2 정규화 강도 조정  

***

## 4. 성능 향상 및 한계  
- **성능 향상**  
  -  *데이터 적을수록:* 낮은 γ로 강한 정규화가 큰 이득 (탭률, 심층 모두)  
  -  *균일한 방문 분포와 느린 mixing:* γ 조정에서 더 큰 효과  
  -  *L2 vs. Discount:* 탭률에선 유사하나, 심층 제어에선 태스크·데이터 양마다 유·무 차이  

- **한계 및 고려사항**  
  1. **편향-분산 트레이드오프**: 지나치게 낮은 γ는 장기 보상을 무시해 편향 증가  
  2. **환경별 γ 최적값 상이**: 경험적 그리드 탐색 필요  
  3. **상태 비방문 구간 무정규화**: 방문 적은 상태는 λ가 작아 과적합 위험  

***

## 5. 향후 연구 영향 및 고려점  
1. **적응형 정규화**: 학습 중 γ와 L2를 자동 조절하는 메타-학습 기법 개발  
2. **이론적 해석 강화**: 데이터 분포·mixing time에 따른 최적 γ 이론적 바운드 제시  
3. **확장성 검증**: 대규모 실세계 POMDP, 비정형 관측에서의 할인 정규화 효과 분석  
4. **복합 정규화 스케줄**: 학습 초반 낮은 γ→후반 γ 상승 방식의 최적 스케줄링 연구  

*요약*  
할인율을 조정하는 간단한 기법이 값 함수의 활성화 크기를 제어하는 효과적인 정규화 수단임을 이론·실험적으로 규명했으며, 제한적 데이터 및 다양한 환경에서 일반화 성능을 크게 향상시킬 수 있음을 보였다. 향후 메타-적응 기법 및 이론적 분석을 통해 실전 RL 시스템의 일반화 강인성을 높이는 데 핵심 기여가 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/019e76bc-5381-4e6a-bec2-2a0f4f3d52e3/2007.02040v1.pdf
