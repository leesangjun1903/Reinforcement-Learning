# Research on the Deep Deterministic Policy Algorithm Based on the First-Order Inverted Pendulum

**주요 결론:**  
개선된 DDPG 알고리즘은 재귀적 소형 경험 풀 구조와 이중 Critic 네트워크를 결합하여 Q값 과대추정을 억제하고, 수렴 속도를 기존 대비 약 38% 단축하며(235→145 에피소드), 다양한 외란 환경에서 향상된 일반화 능력과 더 빠른 안정 회복을 달성한다.

## 1. 핵심 주장 및 주요 기여  
이 논문은 **전통 제어(PID)의 비선형·강결합 시스템 한계**를 극복하기 위해 DDPG 기반 제어기를 설계하고, 다음과 같은 개선 기여를 제안한다.  
1. 재귀적 소형 경험 풀(RSEP-DDPG)  
   -  전체 경험 풀을 n개의 소형 풀로 분할하고, 각 풀의 “유효 경험”을 평가·흡수하여 샘플링 효율을 높인다.  
   -  Soft update 비율 τ를 경험 풀 평가 결과에 따라 조정함으로써(θ′←τθ+(1−τ)θ′), 로컬 옵티마 회피와 훈련 가속을 달성한다.  
2. 이중 Critic 네트워크 구조  
   -  두 개의 타깃 Critic을 사용해 Q 목표를  
     
     $$y_i = r_i + \gamma \min_{j=1,2}Q_{\theta_j'}(s_{i+1},\,a_{i+1})$$  
     
     으로 정의하여(과대추정 억제) 안정성을 높인다.  
   -  타깃 액션에 정규분포 잡음 ε∼clip(N(0,σ),−c,c)를 추가해 overfitting을 방지한다.  
3. 실제 1차 역진자 모델(Matlab/Simulink) 검증  
   -  PID, 기존 DDPG, 개선 DDPG를 비교: 개선 DDPG는 에피소드 당 평균 수렴 보상을 보존하며 학습 속도를 38% 단축하고, 외란 후 안정 회복 시간을 1.388 s로 단축한다.  

## 2. 문제 정의·제안 방법·모델 구조·성능·한계  

### 2.1 해결하고자 하는 문제  
- 전통 PID는 시스템 모델 선형화 기반으로 비선형·강결합 환경에서 성능 저하  
- DDPG는 경험 풀 단순 샘플링과 단일 Critic으로 인해 Q값 과대추정 및 로컬 옵티마 민감  

### 2.2 제안 방법  
-  **재귀적 소형 경험 풀**  
  - 경험 풀을 n개의 소형 풀로 분할하고, 각 풀을 평가해 유효 경험만 재귀적 통합  
  - Soft update 계수 τ를 경험 풀 크기·유효 경험에 따라 동적으로 조정(θQ′←τθQ+(1−τ)θQ′)  
-  **이중 Critic 구조 및 잡음 추가**  
  - 두 개의 타깃 Critic Q₁′, Q₂′ 중 최소값을 목표로 삼아 과대추정 억제:  
    $$y_i = r_i + \gamma\min_{j=1,2}Q_{\theta_j'}(s_{i+1},\,\mu'(s_{i+1})+\epsilon)$$  
    $$\epsilon \sim \mathrm{clip}(\mathcal{N}(0,\sigma),-c,c)$$  

### 2.3 모델 구조  
- **Actor 네트워크**: 입력 5 차원 → 히든 128 → 200 → 출력 1  
- **Critic 네트워크**: 입력(5 차원 상태 + 1 차원 행동) → 히든 128 → 200 → 출력 Q값  
- 하이퍼파라미터: 학습률 (Critic 0.001, Actor 0.0005), 할인율 γ=0.995, τ=0.001, ReLU 활성화, 업데이트 간격 100, 타깃 업데이트 주기 10  

### 2.4 성능 향상  
- **학습 효율**: 235 → 145 에피소드 수렴(38% 단축)  
- **안정성 회복**: 외란 후 안정 복귀 시간 DDPG 1.725 s → 개선 DDPG 1.388 s  
- **과대추정 억제**: 화이트 노이즈 환경에서 최대 각변위 0.0892 rad → 0.0759 rad로 축소  
- **통제 가능한 범위 유지**: 모든 제어 지표(변위·속도·각변위·각속도) 유의미한 범위 이내  

### 2.5 한계  
- **보상 함수 편향**: 각변위 가중치(×5)가 높아져 횡 변위 제어 성능 일부 희생  
- **시뮬레이션 의존**: 현실 환경(sim-to-real) 전이 문제 미해결  
- **하이퍼파라미터 민감도**: τ, σ, 경험 풀 분할 수 등 추가 파라미터 튜닝 필요  

## 3. 일반화 성능 향상 관점  
- **다양한 경험 샘플 확보**: 소형 경험 풀 구조로 드문 경험도 높은 확률로 재생, 드리프트·외란 불확실성 대비  
- **과대추정 억제**: 이중 Critic으로 Q값 편향 감소 → 과적합 완화  
- **추가 개선 제안**: 도메인 랜덤화, 멀티태스크·이종 보상 함수, 선행 데이터 기반 사전 학습(prior experience) 결합  

## 4. 향후 연구 영향 및 고려 사항  
- **강화학습·전통 제어 융합 확대**: 블랙박스 DDPG와 화이트박스 모델 결합한 하이브리드 제어기 설계  
- **Sim-to-Real 전이**: 현실 간극 해소를 위한 도메인 랜덤화, 모형 기반 보정 기법 적용  
- **안전성 및 해석 가능성**: 안전 보장 메커니즘(제약 강화학습), 네트워크 해석성 강화  
- **보상 함수 자동화**: 메타학습·자기 지도 보상 튜닝으로 수동 조정 최소화  

이 논문은 DDPG 제어기의 학습 효율과 안정성, 일반화 성능을 실질적으로 개선한 방법론을 제시하며, 전통 제어 시스템에 강화학습을 적용하는 후속 연구의 기반을 제공한다. 앞으로는 실환경 전이, 보상 자동화, 하이브리드 제어기 설계에 주목해야 할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/c0149547-c959-47b0-8aef-3a04d917ffa4/applsci-13-07594-v2.pdf
