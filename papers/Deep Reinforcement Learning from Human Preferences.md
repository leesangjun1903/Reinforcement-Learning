# Deep Reinforcement Learning from Human Preferences

## 1. 핵심 주장 및 주요 기여
“Deep Reinforcement Learning from Human Preferences”는 인간이 제공하는 선호(preference) 피드백만으로 복잡한 강화학습 과제를 해결할 수 있음을 보인다.  
- **핵심 주장**: 보상 함수가 명시적으로 주어지지 않아도, 인간이 두 행동 궤적을 비교한 짧은 동영상 클립 선호 정보만으로 딥 RL 에이전트를 효과적으로 훈련할 수 있다.  
- **주요 기여**:  
  1. 인간 선호를 학습한 보상 예측기를 강화학습에 결합하여, 총 상호작용의 1% 미만의 피드백만으로 Atari 게임 및 MuJoCo 로봇 제어 과제를 성공적으로 수행.  
  2. 비전문가 사용자의 1시간 이내 피드백으로 새로운 복잡 행동(백플립, 교통 흐름 준수 등) 학습 시연.  
  3. 비교 기반 선호 학습(scale-up) 아키텍처 및 효율적 쿼리 선택 전략 제안.

## 2. 문제 정의 및 제안 방법
### 2.1 해결 과제
일반 강화학습은 명시적 보상 함수 $$r(o,a)$$가 필요하지만, 실제 환경에서는  
- 목표가 복잡·불명확하거나  
- 적절한 보상 설계가 어려울 때  
정책이 원치 않는 방향으로 최적화될 위험이 있다.

### 2.2 제안된 방법
1. **보상 예측기 $$\hat r(o,a)$$**  
   - 인간이 비교한 궤적 세그먼트 $$\sigma_1, \sigma_2$$에 대해, 선호 확률을 Bradley-Terry 모델로 예측:  

$$
       \hat P(\sigma_1 \succ \sigma_2)
       = \frac{\exp\sum_t \hat r(o^1_t,a^1_t)}{\exp\sum_t \hat r(o^1_t,a^1_t) + \exp\sum_t \hat r(o^2_t,a^2_t)}
     $$
   
   - 교차 엔트로피 손실을 최소화하며 앙상블·정규화·드롭아웃으로 일반화 강화.
2. **쿼리 선택**  
   - 앙상블 예측기 간 불확실도가 높은 궤적 쌍을 우선 선택(variance-based).
3. **정책 최적화**  
   - $$\hat r$$로 평가된 보상에 기반해 A2C·TRPO 알고리즘으로 정책 업데이트.
4. **비동기 파이프라인**  
   - (1) 정책 롤아웃 → (2) 인간 비교 피드백 → (3) 보상 예측기 학습 → (1) 로 연속 수행.

### 2.3 모델 구조
- **보상 예측기 네트워크**:  
  - MuJoCo: 2-layer MLP(각 64 유닛, leaky ReLU)  
  - Atari: 4개의 컨볼루션 층 + FC64 + 출력 스칼라, 배치정규화·드롭아웃 적용  
- **정책 네트워크**:  
  - Atari: 표준 A3C CNN 아키텍처  
  - MuJoCo: 상태 입력 MLP 기반 TRPO

## 3. 성능 향상 및 한계
### 3.1 성능
- **MuJoCo 제어**: 700회 인간 쿼리만으로 진짜 보상 학습(TRPO) 성능 근접 혹은 초과  
- **Atari 게임**: 5,500회 쿼리로 BeamRider·Pong 등 일부 게임에서 RL 성능 매칭, 일부 게임은 느리게 수렴  
- **새 행동 학습**: Hopper 백플립, Cheetah 한 다리 전진, Enduro 차 흐름 유지 등 1시간 내 성공적 학습

### 3.2 한계
- **피드백 효율성**: Qbert 등 복잡 시각 환경에서 인간이 짧은 클립만으로 정확히 비교하기 어려움  
- **비정상적 탐색**: 보상 예측 오차로 인해 비정상적 행동(끝없는 랠리 등) 발생 가능  
- **오프라인 학습 취약**: 고정된 비교 데이터만으로는 분포 변화 따라가지 못해 불안정

## 4. 일반화 성능 향상 가능성
- **앙상블 및 정규화**: 다양한 예측 모델 간 불확실도 추정으로 과적합 저감  
- **온라인 쿼리**: 정책 변화에 맞춰 피드백 분포 적응, 비정상적 행동 억제  
- **클립 길이 최적화**: 1–2초 클립이 평가 시간 대비 정보량 균형 최적  
- **향후 개선**:  
  1. 적극적 정보획득(expected information gain) 기반 쿼리 전략  
  2. 도메인 불변 표현 학습으로 시각적 일반화 강화  
  3. 휴리스틱 리워드 셰이핑과 병합하여 초기 탐색 가속

## 5. 향후 연구 영향 및 고려점
이 논문은 *인간 선호 기반 보상 학습*을 대규모 딥 RL에 적용 가능함을 보임으로써,  
- **실제 로봇, 자율 시스템** 등에 복잡 목표 전달 메커니즘 제시  
- **AI 안전·정합성(alignment)** 연구에 새로운 방향 제시  
향후 연구 시 고려할 점은,  
1. **피드백 비용 최소화**: 보다 효과적인 쿼리 설계 및 자동화된 피드백 보완 방법  
2. **다양한 도메인 확장성**: 시각·언어·물리 환경 전반에 걸친 일반화 성능 검증  
3. **인간–기계 상호작용 인체공학**: 비전문가가 일관된 선호를 제공할 수 있도록 UI/UX 개선  
4. **안정성 및 안전 확보**: 보상 예측 오류에 따른 허위 목표 추구 방지 메커니즘 개발

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/5c4c3c8b-f4b3-4c8f-971b-abea79717c3d/1706.03741v4.pdf)
