# Distral: Robust Multitask Reinforcement Learning

## 1. 핵심 주장과 주요 기여

Distral(Distill & Transfer Learning)은 **멀티태스크 강화학습에서 태스크 간 지식 전이를 효과적으로 수행하기 위한 새로운 프레임워크**입니다. 이 논문의 핵심 주장은 파라미터 공유 방식 대신 **"증류된 정책(distilled policy)"을 공유**함으로써 태스크 간 공통 행동 패턴을 추출하고 전이할 수 있다는 것입니다.[1]

주요 기여는 다음과 같습니다:

**방법론적 혁신**: 각 태스크별 정책($$\pi_i$$)은 자신의 태스크를 해결하면서 동시에 공유 정책($$\pi_0$$)에 가까이 유지되도록 KL divergence로 정규화되며, 공유 정책은 모든 태스크 정책들의 중심(centroid)이 되도록 증류(distillation) 방식으로 학습됩니다.[1]

**성능 개선**: 복잡한 3D 환경(DeepMind Lab)에서 기존 A3C 기반 멀티태스크 학습 방법들보다 **더 빠른 학습 속도, 더 높은 최종 성능, 그리고 하이퍼파라미터에 대한 더 높은 강건성**을 보였습니다.[1]

**안정성 향상**: 기존 멀티태스크 학습에서 발생하는 태스크 간 그래디언트 간섭 문제와 한 태스크가 학습을 지배하는 문제를 해결하여 더 안정적인 학습이 가능합니다.[1]

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 해결하고자 하는 문제

딥 강화학습은 복잡한 환경에서 데이터 효율성이 낮으며, 멀티태스크 학습을 통해 관련 태스크 간 전이를 통해 효율성을 개선할 수 있지만, 실제로는 다음과 같은 문제들이 발생합니다:[1]

- **그래디언트 간섭**: 서로 다른 태스크의 그래디언트가 노이즈처럼 작용하여 학습을 방해하고 불안정하게 만듭니다
- **태스크 지배 문제**: 태스크 간 보상 체계 차이로 인해 한 태스크가 공유 모델 학습을 지배할 수 있습니다
- **부정적 전이**: 멀티태스크 학습이 오히려 개별 태스크 성능을 저하시키는 경우가 빈번합니다

### 제안 방법 (수식 포함)

Distral의 핵심은 **결합 목적 함수(joint objective function)** 최적화입니다:[1]

$$J(\pi_0, \{\pi_i\}_{i=1}^n) = \sum_i \mathbb{E}_{\pi_i}\left[\sum_{t \geq 0} \gamma^t R_i(a_t, s_t) - c_{KL}\gamma^t \log \frac{\pi_i(a_t|s_t)}{\pi_0(a_t|s_t)} - c_{Ent}\gamma^t \log \pi_i(a_t|s_t)\right]$$

이를 재정리하면:

$$J(\pi_0, \{\pi_i\}_{i=1}^n) = \sum_i \mathbb{E}_{\pi_i}\left[\sum_{t \geq 0} \gamma^t R_i(a_t, s_t) + \gamma^t\frac{\alpha}{\beta}\log \pi_0(a_t|s_t) - \frac{\gamma^t}{\beta} \log \pi_i(a_t|s_t)\right]$$

여기서 $$\alpha = c_{KL}/(c_{KL} + c_{Ent})$$, $$\beta = 1/(c_{KL} + c_{Ent})$$입니다.[1]

**Soft Q-Learning과 증류**: $$\pi_0$$가 고정되었을 때, 각 태스크에 대해 softened Bellman 업데이트를 수행합니다:[1]

$$V_i(s_t) = \frac{1}{\beta} \log \sum_{a_t} \pi_0^{\alpha}(a_t|s_t) \exp[\beta Q_i(a_t, s_t)]$$

$$Q_i(a_t, s_t) = R_i(a_t, s_t) + \gamma \sum_{s_t} p_i(s_{t+1}|s_t, a_t)V_i(s_{t+1})$$

최적 태스크 정책은 Boltzmann 형태를 가집니다:[1]

$$\pi_i(a_t|s_t) = \pi_0^{\alpha}(a_t|s_t)e^{\beta A_i(a_t|s_t)}$$

여기서 $$A_i(a, s) = Q_i(a, s) - V_i(s)$$는 softened advantage function입니다.[1]

**증류 정책 학습**: 증류 정책 $$\pi_0$$는 모든 태스크 정책들의 혼합 분포에 대한 최대 우도 추정으로 학습됩니다:[1]

$$\frac{\alpha}{\beta}\sum_i \mathbb{E}_{\pi_i}\left[\sum_{t \geq 0} \gamma^t \log \pi_0(a_t|s_t)\right]$$

**엔트로피 정규화의 역할**: $$\alpha < 1$$일 때 추가 엔트로피 항이 포함되어, 정책이 탐욕적(greedy)으로 수렴하는 것을 방지하고 충분한 탐색을 보장합니다. 이는 한 태스크가 먼저 해결되었을 때 다른 태스크의 탐색이 중단되는 것을 방지합니다.[1]

### 모델 구조

**Two-Column Architecture**: Distral은 두 가지 아키텍처를 제안합니다:[1]

1. **Single Column**: 각 태스크 정책과 증류 정책이 별도의 네트워크로 파라미터화
2. **Two Column**: 증류 정책과 태스크별 조정을 결합한 구조

Two-column 구조에서 증류 정책은 다음과 같이 파라미터화됩니다:[1]

$$\hat{\pi}_0(a_t|s_t) = \frac{\exp(h_{\theta_0}(a_t|s_t))}{\sum_{a'} \exp(h_{\theta_0}(a'|s_t))}$$

Soft advantage는 다음과 같이 추정됩니다:[1]

$$\hat{A}_i(a_t|s_t) = f_{\theta_i}(a_t|s_t) - \frac{1}{\beta}\log \sum_a \hat{\pi}_0^{\alpha}(a|s_t)\exp(\beta f_{\theta_i}(a|s_t))$$

태스크 정책은 다음과 같은 형태를 가집니다:[1]

$$\hat{\pi}_i(a_t|s_t) = \frac{\exp(\alpha h_{\theta_0}(a_t|s_t) + \beta f_{\theta_i}(a_t|s_t))}{\sum_{a'} \exp(\alpha h_{\theta_0}(a'|s_t) + \beta f_{\theta_i}(a'|s_t))}$$

**Policy Gradient**: 증류 정책 $$\theta_0$$에 대한 그래디언트는 두 항으로 구성됩니다:[1]

$$\nabla_{\theta_0} J = \sum_i \mathbb{E}_{\hat{\pi}_i}\left[\sum_{t \geq 1} \nabla_{\theta_0} \log \hat{\pi}_i(a_t|s_t)\left(\sum_{u \geq t} \gamma^u R_i^{reg}(a_u, s_u)\right)\right] + \frac{\alpha}{\beta}\sum_i \mathbb{E}_{\hat{\pi}_i}\left[\sum_{t \geq 1} \gamma^t \sum_{a'_t} (\hat{\pi}_i(a'_t|s_t) - \hat{\pi}_0(a'_t|s_t))\nabla_{\theta_0} h_{\theta_0}(a'_t|s_t)\right]$$

두 번째 항은 증류 정책이 **모든 태스크 정책들의 중심(centroid)**이 되도록 유도하며, 이는 ADMM, elastic-averaging SGD와 유사하지만 **파라미터 공간이 아닌 정책 공간에서 작동**한다는 중요한 차이가 있습니다.[1]

**네트워크 구조**: DeepMind Lab 실험에서는 A3C와 동일한 아키텍처를 사용했으며, 2개의 컨볼루션 레이어(ReLU), 256 hidden units의 fully connected layer(ReLU), LSTM으로 구성됩니다.[1]

### 성능 향상

**Grid World 실험**: 2개의 방이 복도로 연결된 그리드 월드에서 Distral은 단일 태스크 학습보다 **훨씬 빠르게 학습**하고 **더 나은 정책으로 수렴**했습니다. 증류 정책은 복도에서 일관되게 이동하는 강건한 행동을 학습하여 탐색을 도왔습니다.[1]

**Mazes 실험** (8개 태스크): 
- Distral 알고리즘들(KL_2col, KL+ent_2col)은 **모든 A3C 베이스라인보다 빠르게 학습**하고 **더 높은 최종 성능**을 달성했습니다
- Two-column 알고리즘이 single-column보다 빠르게 학습했습니다
- Distral 알고리즘은 **훨씬 더 안정적**이며, 일부 실행에서만 학습하는 A3C 멀티태스크와 달리 대부분의 실행에서 좋은 성능을 보였습니다
- 36개 실행 중 **KL+ent_2col이 가장 강건**했습니다[1]

**Navigation 실험** (4개 태스크):
- 절차적으로 생성되는 복잡한 맵에서 Distral 알고리즘이 **더 나은 최종 결과와 안정성**을 보였습니다
- Two-column Distral 알고리즘(KL_2col, KL+ent_2col)이 최고 성능을 기록했습니다[1]

**Laser-tag 실험** (8개 태스크):
- 태스크 간 다양성이 높은 경우, **single-column Distral이 더 나은 성능**을 보였습니다
- 베이스라인 A3C와 비슷한 수준이었지만, **멀티태스크 A3C보다는 확실히 우수**했습니다[1]

**하이퍼파라미터 강건성**: Distral 알고리즘은 다양한 하이퍼파라미터 설정에서 일관되게 높은 성능을 보여 실용성이 높습니다.[1]

### 한계

논문에서 명시적으로 언급된 한계들은:

**태스크 다양성이 높을 때의 제한**: Laser-tag 실험에서 태스크 간 차이가 클 때, Distral이 베이스라인 A3C를 크게 능가하지 못했습니다. 이는 태스크들이 매우 다를 때 공통 정책을 학습하는 것이 어려울 수 있음을 시사합니다.[1]

**초기 plateau 현상**: 엔트로피 항이 없는 알고리즘(KL_1col, KL_2col)은 초기에 빠르게 학습하지만 **조기에 plateau에 도달**하여 최종 성능이 낮았습니다. 이는 충분한 탐색 없이 수렴하기 때문입니다.[1]

**하이퍼파라미터 튜닝**: 논문에서는 $$\alpha = 0.5$$로 고정했으며, 최적의 KL과 엔트로피 정규화 균형은 태스크 특성에 따라 달라질 수 있습니다.[1]

## 3. 일반화 성능 향상 가능성

Distral은 여러 메커니즘을 통해 **일반화 성능을 크게 향상**시킵니다:

### 공통 행동 패턴의 증류

증류 정책 $$\pi_0$$는 **여러 태스크에서 공통적으로 나타나는 유용한 행동 패턴**을 추출합니다. Grid world 실험에서 증류 정책은 복도에서 일관되게 이동하는 행동을 학습했으며, 이는 새로운 테스트 태스크에서도 **더 빠른 탐색과 학습**을 가능하게 했습니다.[1]

### 보상 형성(Reward Shaping) 효과

증류 정책은 $$\log \pi_0(a|s)$$ 항을 통해 **보상 형성 효과**를 제공합니다. 이는 높은 확률을 가진 행동을 장려하여 무작위 탐색의 병목을 극복하고, 한 태스크에서 얻은 지식이 다른 태스크로 전이되도록 합니다[1].

### 정책 공간에서의 정규화

파라미터 공간이 아닌 **의미론적으로 더 의미 있는 정책 공간**에서 정규화가 이루어져, 학습을 안정화하고 일반화를 촉진합니다. 증류 정책이 모든 태스크 정책들의 중심이 되도록 학습되므로, **태스크 간 부정적 간섭을 줄이고 긍정적 전이를 촉진**합니다.[1]

### 탐색-전이 균형 제어

$$\alpha$$ 파라미터를 통해 KL 정규화(전이)와 엔트로피 정규화(탐색)의 균형을 **독립적으로 제어**할 수 있습니다. 이는 새로운 태스크를 접했을 때 충분한 탐색을 보장하면서도 기존 지식을 활용할 수 있게 합니다.[1]

### 실험적 증거

**증류 정책의 전이 능력**: Maze 실험에서 증류 정책 자체가 개별 태스크에서도 좋은 성능을 보였으며, 특히 KL_2col이 가장 높은 성능을 달성했습니다. 이는 증류 정책이 **태스크에 구애받지 않는 일반화된 지식**을 학습했음을 보여줍니다.[1]

**새로운 태스크로의 확장**: Grid world 테스트 태스크 실험에서 Distral로 학습된 에이전트가 **학습 중 보지 못한 새로운 목표 위치에서도 더 빠르게 학습**했습니다. 이는 증류 정책이 제공하는 일반화된 행동이 새로운 상황에 효과적으로 전이됨을 증명합니다.[1]

**연속 학습(Continual Learning) 가능성**: Distral의 구조는 태스크가 순차적으로 주어지는 연속 학습 시나리오에도 적용 가능합니다. 증류 정책이 이전 태스크들의 지식을 보존하면서 새로운 태스크에 적응할 수 있습니다.[1]

## 4. 향후 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향

**멀티태스크 강화학습 패러다임 변화**: 파라미터 공유에서 **행동/정책 공유**로의 전환을 제시하여, 멀티태스크 강화학습의 새로운 연구 방향을 열었습니다.[1]

**증류 기법의 재해석**: 기존 지식 증류(knowledge distillation)를 단방향 전이가 아닌 **양방향 반복적 프로세스**로 확장하여, 증류 정책이 다시 태스크 정책 학습에 피드백되는 메커니즘을 도입했습니다.[1]

**안정성과 강건성**: Distral이 보여준 높은 안정성과 하이퍼파라미터 강건성은 **실용적인 딥 강화학습 시스템 구축**에 중요한 시사점을 제공합니다.[1]

**전이 학습의 이론적 토대**: RL을 확률적 추론(probabilistic inference)으로 보는 관점에서 정책 prior로서의 증류 정책 개념을 발전시켜, 이론적 기반을 강화했습니다.[1]

### 향후 연구 시 고려사항

**보조 손실과의 결합**: Distral을 UNREAL 등의 보조 태스크 기법과 결합하면 데이터 효율성을 더욱 향상시킬 수 있습니다.[1]

**다중 증류 정책**: 현재는 단일 증류 정책을 사용하지만, **여러 증류 정책이나 latent variable**을 도입하여 더 다양한 행동을 표현할 수 있습니다.[1]

**연속 학습 시나리오**: 태스크가 순차적으로 제공되는 상황에서 Distral을 적용하고, catastrophic forgetting을 방지하는 메커니즘을 연구할 필요가 있습니다.[1]

**적응적 정규화**: KL과 엔트로피 비용($$c_{KL}$$, $$c_{Ent}$$)을 학습 과정에서 **적응적으로 조정**하여 전이와 탐색의 균형을 더 효과적으로 제어하는 방법을 탐구해야 합니다.[1]

**태스크 다양성 처리**: 매우 이질적인 태스크들에 대해서는 **태스크 클러스터링**이나 **계층적 증류 정책** 구조를 고려할 수 있습니다. Fox et al.의 option learning 접근을 확장하여 딥러닝에 적용하는 것도 가능합니다.[1]

**태스크별 하이퍼파라미터**: 태스크 간 보상 스케일과 탐색 필요성이 크게 다를 때, 태스크별 정규화 비용을 사용하는 것이 유리할 수 있지만, 이는 최적화할 하이퍼파라미터를 증가시킵니다.[1]

**확장성**: 더 많은 수의 태스크와 더 복잡한 환경에서 Distral의 성능을 검증하고, 분산 학습 효율성을 개선하는 연구가 필요합니다.

**이론적 분석**: Distral의 수렴 특성, 전이 효율성에 대한 이론적 보장, 그리고 최적의 $$\alpha$$ 선택에 대한 이론적 지침을 제공하는 연구가 요구됩니다.

Distral은 멀티태스크 강화학습에서 **파라미터 공유의 한계를 극복하고 안정적이고 효율적인 지식 전이**를 가능하게 한 중요한 기여로, 향후 강화학습 연구에서 일반화 성능 향상과 실용적 시스템 구축에 광범위하게 영향을 미칠 것으로 기대됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ffb456cc-2b20-4c9d-80e7-df224c29fba9/1707.04175v1.pdf)
