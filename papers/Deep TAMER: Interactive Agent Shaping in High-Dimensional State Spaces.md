# Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces

## 1. 핵심 주장 및 주요 기여  
**Deep TAMER**는 기존 TAMER 프레임워크를 고차원 상태 공간(예: 픽셀 기반 이미지)에도 적용할 수 있도록 확장한 기법이다.  
- **핵심 주장**: 사람의 실시간 스칼라 피드백을 딥 뉴럴 네트워크로 모델링함으로써, 제한된 학습 시간과 데이터만으로도 에이전트가 복잡한 환경에서 효율적으로 행동 정책을 학습할 수 있다.  
- **주요 기여**:  
  1. **딥 리워드 모델**: 인간 피드백 함수를 근사하는 CNN-기반 모델(Deep Reward Model) 제안  
  2. **오토인코더 프리트레이닝**: 고차원 상태 특징 추출을 위한 CNN 전처리부를 시뮬레이션 상태 데이터로 오토인코더 학습  
  3. **피드백 리플레이 버퍼**: 사람 피드백의 희소성을 극복하기 위해 경험-피드백 쌍을 버퍼에 저장·재사용  

## 2. 문제 정의 및 제안 방법  

### 문제 정의  
에이전트는 상태 집합 $$S$$와 행동 집합 $$A$$에서 시퀀셜 의사결정을 수행하며, 인간 트레이너는 스칼라 평가 $$h$$를 실시간 제공한다. 인간의 평가는 숨겨진 함수 $$H(s,a)$$에 기반하며, 에이전트는 이를 근사하는 $$\hat H(s,a)$$를 온라인으로 학습한다.  

### 손실 함수 및 최적화  
각 상태-행동 쌍 $$(s,a)$$와 피드백 $$h$$에 대해 중요도 가중치 $$w$$를 곱한 가중 제곱 오차로 정의:  

$$
\ell(\hat H;x,y) = w(ts,te,tf)\,\bigl(\hat H(s,a)-h\bigr)^2
$$  

여기서 $$w$$는 피드백 시점 $$t_f$$가 행동 수행 구간 $$[t_s,t_e]$$에 얼마나 근접한지 반영하는 중요도 함수다(식 (4)). 에이전트는 미니배치 SGD를 통해  

$$
\hat H_{k+1} = \hat H_k - \eta_k \nabla_{\hat H}\,\ell(\hat H_k;x_{i_k},y_{j_k})
$$  

를 반복 수행한다.  

### 딥 리워드 모델 구조  
1. **오토인코더 프리트레이닝**  
   - 입력: 최근 2개 프레임(160×160 그레이스케일)  
   - CNN 인코더 $$f(s;\theta_f)$$: 76,035개 파라미터 → 잠재 차원 $$p=100$$  
   - 대칭 구조의 디코더 $$g(\cdot;\theta_g)$$와 함께 재구성 오차 최소화로 $$\theta_f^*,\theta_g^*$$ 학습  

```math
   (\theta_f^*,\theta_g^*)=\arg\min_{\theta_f,\theta_g}\frac1M\sum_{i=1}^M\|s_i - g(f(s_i;\theta_f);\theta_g)\|^2
```

2. **피드포워드 네트워크 $$z$$**  
   - 인코더 출력 $$f(s)$$와 행동 벡터를 입력으로 2개 은닉층(각 16유닛), 행동별 출력 노드 구성  
   - $$\hat H(s,a)=z(f(s),a)$$  

### 피드백 리플레이 버퍼  
- 새로운 피드백 발생 시 즉시 SGD 업데이트  
- 고정 주기로 버퍼 내 $$(x,y)$$ 샘플 다중 재생 SGD 수행 → 희소 피드백 보완  

## 3. 성능 향상 및 한계  

### 성능 향상  
- **학습 속도**: 인간 트레이너 15분 학습만으로 Atari BOWLING에서 평균 100점 이상 달성  
- **기존 기법 대비**:  
  - D-DQN/A3C 등 딥 RL 기법은 동일 시간 내 유의미 학습 불가  
  - 원조 TAMER(선형 모델)도 픽셀 입력에 부적합하여 학습 실패  
  - 인간 데모 기반 DQfD 대비 우위: 트레이닝 데이터 생성 ×, 사람 비판 피드백 활용  

### 한계  
- **하이퍼파라미터 튜닝 부족**: 중요도 분포, 네트워크 구조 등 세부 설정 고정  
- **도메인 제한**: Bowling 게임만 실험, 다른 환경에서 일반화 검증 필요  
- **실시간 제약**: 사람 피드백 특성에 따른 지연과 불확실성  

## 4. 일반화 성능 향상 가능성  
- **오토인코더 전처리**는 다양한 시각적 환경에서도 잠재 표현 학습에 유용  
- **피드백 리플레이 버퍼**는 피드백 희소성 일반적 문제 해결에 기여  
- 다른 환경에선 **중요도 가중치 함수 $$w$$** 및 **네트워크 용량** 조정이 필수  
- **도메인 어댑테이션**: 사전학습된 인코더를 전이 학습으로 활용하면 소규모 피드백으로도 빠른 적응 가능  

## 5. 향후 연구 영향 및 고려 사항  
Deep TAMER는 인간과의 상호작용을 통한 샘플 효율적 강화학습 연구에 새로운 길을 열었다.  
- **영향**:  
  - 실시간 인간 피드백을 고차원 상태에 안정적으로 적용  
  - 휴먼-인-더-루프 학습 프레임워크 발전  
- **고려할 점**:  
  1. **다양한 환경 확장**: 복잡한 로보틱스, 자율주행 등 실환경에서의 적용 검증  
  2. **피드백 편향 해소**: 사람마다 다른 피드백 스타일 / 신뢰도 자동 보정  
  3. **하이퍼파라미터 자동화**: 중요도 분포, 네트워크 크기 최적화 기법 도입  
  4. **장기 일반화**: 사전학습 인코더 전이 / 메타러닝으로 빠른 신규 환경 적응  

---  
Deep TAMER는 제한된 인간 피드백만으로도 딥 네트워크를 효율적으로 학습시켜, 고차원 상태 공간에서의 강화학습 문제를 실시간으로 해결할 수 있음을 입증하였다. 앞으로 고차원·희소 보상 환경에서의 휴먼-인-더-루프 학습 연구가 활발히 확장될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d57074b3-ca74-4e45-bbfa-cb0194a72e6d/1709.10163v2.pdf)
