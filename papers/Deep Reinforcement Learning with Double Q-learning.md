# Deep Reinforcement Learning with Double Q-learning

**핵심 주장 및 기여**  
Deep Q-Networks(DQN)가 게임 환경 등에서 학습 중 과도한 행동 가치(overestimation)를 일으키며 성능을 저해하는 문제를 경험적으로 입증하고, 기존 표 형식(tabular)의 Double Q-learning 개념을 대규모 함수 근사(딥 뉴럴 네트워크) 환경에 적용한 **Double DQN** 알고리즘을 제안한다. 주요 기여는 다음과 같다:[1]
- DQN의 과대추정 현상이 실제 확률적·비확률적 환경 모두에서 빈번하고 심각함을 확인.  
- 행동 선택(selection)과 평가(evaluation)를 분리함으로써 과대추정을 감소시키는 Double Q-learning 이론을 일반 함수 근사 setting으로 확장.  
- 기존 DQN 아키텍처에 최소한의 변경(타깃 네트워크 활용)만으로 구현한 Double DQN을 제시.  
- Atari 2600 벤치마크에서 더 정확한 가치 추정과 우수한 정책 성능을 달성, 학습 안정성 및 평균 점수 대폭 향상.

## 문제 정의  
Q-learning의 최대화 연산(max)은 추정 오차가 섞인 값들 중에서 과대추정된 항목을 선택하는 경향이 있어, 비균일(overestimated) 오차가 정책 학습 질을 저하시킬 수 있다.[1]
- **과대추정 원인**: 함수 근사 오류, 환경 노이즈, 비정상성 등 모든 형태의 추정 에러가 합쳐져서 발생.  
- **실제 영향**: DQN이 Atari 게임 학습 시 진짜 누적 보상(actual return)에 비해 과도하게 높은 Q값을 예측함으로써 불안정하고 부진한 정책을 학습.

## 제안 방법  
### Double Q-learning 일반화  
표 형식의 Double Q-learning 오차 목표식(target)을  

$$
Y^\text{DoubleQ}_t = R_{t+1} + \gamma\,Q\bigl(S_{t+1},\,\arg\max_a Q(S_{t+1},a;\theta_t)\;;\theta^-_t\bigr)
$$  

로 정의하여 선택(selection)은 온라인 네트워크 $$\theta_t$$로, 평가(evaluation)는 별도 네트워크 $$\theta^-_t$$로 수행한다.[1]

### Double DQN 알고리즘  
DQN의 타깃 네트워크를 활용해 두 네트워크를 마련하고, 업데이트 타깃을  

$$
Y^\text{DoubleDQN}_t = R_{t+1} + \gamma\,Q\bigl(S_{t+1},\,\arg\max_a Q(S_{t+1},a;\theta_t)\;;\theta^-_t\bigr)
$$  

로 교체한다. 이 외 경험 재플레이와 네트워크 구조, 하이퍼파라미터는 DQN과 동일하다.

### 모델 구조  
- 입력: 전처리된 84×84 그레이스케일 프레임 4장  
- 합성곱층 3개 → 완전연결층 1개 → 출력층(Q값 벡터)  
- 총 파라미터 약 1.5M  
- RMSProp 최적화, 할인율 $$\gamma=0.99$$, 타깃 네트워크 업데이트 주기 10k 스텝  

## 성능 향상  
- **가치 추정 정확도**: DQN 대비 과대추정 편향이 현저히 감소하여, 실제 정책의 누적 보상(actual return)과 예측값이 일치에 가까워짐.[1]
- **정책 품질**: Atari 49개 게임에서 DQN 대비 중간값(median) 성능이 93.5→114.7, 평균(mean)이 241.1→330.3으로 대폭 향상.[1]
- **학습 안정성**: 불안정하게 진동하던 DQN과 달리 Double DQN은 학습 곡선이 매끄럽고 안정적.  

## 한계  
- **과대추정 완전 제거 불가**: 타깃 네트워크를 공유함으로써 두 네트워크가 완전히 독립적이지 않아 일부 과대추정이 잔존할 수 있음.  
- **탐험-이용 균형**: 과대추정을 줄인 만큼 탐험(optimal exploration)이 소극적으로 이루어질 위험이 있으며, 추가적인 보상 보정 기법 필요성 대두.  
- **하이퍼파라미터 민감도**: 타깃 네트워크 교체 주기나 학습률 등은 여전히 경험적으로 튜닝해야 함.

## 일반화 성능 관점  
Double DQN은 **휴먼 스타트(human starts)** 평가(무작위 시작 상태)에서도 DQN 대비 평균 및 중앙 성능이 크게 향상되어, 결정적 환경에서 패턴 암기에 그치지 않고 다양한 초기 상태에 대한 **강건한 일반화**를 확인했다. 이는 단일 반복궤적(sequence memorization) 해결 방식이 아니라 정책 자체가 환경 특성을 포괄적으로 학습했음을 시사한다.[1]

## 향후 연구에의 영향 및 고려사항  
향후 강화학습 연구에서 **과대추정 완화(bias reduction)** 메커니즘은 표준 기법으로 자리잡을 것이며, 다음 사항들을 고려할 필요가 있다:  
- **다중 네트워크 활용**: 완전 분리된 평가 네트워크, 앙상블 방법 등을 통해 추가적인 편향 감소 연구  
- **보상 추정 불확실성 반영**: 베이지안 접근이나 분포적 Q-learning(distributional RL)과 결합하여 불확실성 평가 강화  
- **탐험 전략 통합**: UCB, 파라미터 노이즈(parameter noise) 등 탐험 보너스와 bias-reduction 기법 간 균형  
- **이론적 수렴성 보장**: Off-policy 딥 RL의 수렴 조건 및 안정성 분석 강화  

이 논문은 강화학습의 편향 문제에 대한 실질적 개선을 제시함으로써, 안정적이고 일반화 가능한 딥 RL 알고리즘 설계 방향을 제시했다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/6bcc00af-26c0-47c0-841e-689bbd56d0bc/1509.06461v3.pdf)
