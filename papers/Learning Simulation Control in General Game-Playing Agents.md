# Learning Simulation Control in General Game-Playing Agents

**핵심 주장 및 주요 기여**  
이 논문은 **일반 게임 플레이(General Game Playing; GGP) 에이전트를 위한 시뮬레이션 제어 학습 기법**을 제안한다. 전통적 MCTS(몬테카를로 트리 탐색)가 평가 함수 없이도 다양하게 적용 가능한 장점을 가지지만, 효과적인 시뮬레이션 가이드가 없으면 성능이 떨어진다. 따라서 논문은 다음의 다섯 가지 **도메인 독립적 시뮬레이션 제어 학습 기법**을 비교·제안한다.  
1. MAST (Move-Average Sampling Technique)  
2. TO-MAST (Tree-Only MAST)  
3. PAST (Predicate-Average Sampling Technique)  
4. RAVE (Rapid Action Value Estimation)  
5. FAST (Features-to-Action Sampling Technique)

또한, 이들 기법을 조합한 **RAVE+MAST**와 **RAVE+FAST**를 제시하여 각 기법의 강점을 상호 보완함으로써 성능을 더욱 향상시켰다.

***

## 1. 해결하고자 하는 문제  
- GGP 에이전트는 사전에 정의된 평가 함수 없이 다양한 게임에 대응해야 함.  
- MCTS 기반 에이전트의 **플레이아웃(playout) 단계**와 **선택(selection) 단계**에서 무작위 또는 사소한 편향만으로는 복잡한 게임에서 충분한 성능을 내기 어려움.

***

## 2. 제안하는 방법  

### 2.1 MCTS 기본 구조  
1) Selection: UCT 공식을 이용해 트리 내에서 행동 선택  

$$
a^* = \arg\max_{a\in A(s)} \Bigl(Q(s,a) + C\,\sqrt{\frac{\ln N(s)}{N(s,a)}}\Bigr)
$$  

2) Playout: 트리 외 구간에서는 무작위 또는 편향된 정책 사용  
3) Expansion: 시뮬레이션 결과 지점에서 새 노드 추가  
4) Back-propagation: 시뮬레이션 결과를 루트까지 전파  

### 2.2 개별 시뮬레이션 제어 기법  
- **MAST**: 모든 시뮬레이션에서 행동 $$a$$에 대한 전체 평균 승률 $$Q_h(a)$$을 학습. Gibbs 분포로 플레아웃 편향:  

$$
  P(a) = \frac{e^{Q_h(a)/\tau}}{\sum_b e^{Q_h(b)/\tau}}
  $$  

- **TO-MAST**: MAST와 동일하나, 트리 구간에서만 $$Q_h(a)$$ 업데이트.  
- **PAST**: 상태의 술어(predicate)와 행동 쌍 $$(p,a)$$에 대한 평균 값 $$Q_p(p,a)$$ 학습. 플레아웃 시 최대값 기반 편향.  
- **RAVE**: 시뮬레이션 중 발생한 모든 후속 행동을 조기 학습하여 트리 선택 단계에 가중치로 결합.  

$$
  \beta(s)\,Q_\text{RAVE}(s,a) + [1-\beta(s)]\,Q(s,a),
  \quad \beta(s)=\frac{k}{3n(s)+k}
  $$  

- **FAST**: 게임 설명 언어(GDL)로부터 추론한 **특징(feature)**(말 유형·위치) 기반 TD(λ) 학습으로 가치 함수 $$V(s)=\theta^\top f(s)$$ 학습. 이 값으로 행동 가치 $$Q(a)$$ 매핑하여 플레이아웃 편향.  

### 2.3 기법 조합  
- **RAVE+MAST (RM)**, **RAVE+FAST (RF)**: 선택 단계(RAVE)와 플레이아웃 단계(MAST/FAST)를 결합하여 전 영역 성능 향상.

***

## 3. 성능 향상 및 한계  

| 게임       | 기본 MCTS 대비 향상율                         |
|-----------|--------------------------------------------|
| Breakthrough | 최대 +90% 승률 (MAST)                         |
| Checkers     | TO-MAST: +82% 승률                     |
| Othello      | FAST: +70% 승률                         |
| Skirmish     | FAST: +96% 승률 (독보적 효과)           |

- **조합 기법(RM, RF)**은 Othello 등에서 개별 기법을 뛰어넘는 성능을 보임.  
- **한계**:  
  - PAST의 술어 기반 일반화는 상태 수가 많아지면 연산 오버헤드(최대 20%) 발생.  
  - FAST는 체스류 게임에 특화되지만, 다른 도메인 일반화에는 추가 연구 필요.

***

## 4. 모델의 일반화 성능 향상 가능성  

- **FAST**가 게임 구조(말 유형, 셀 위치)로부터 특징을 자동 추론하여 학습하는 방식은 **다양한 보드 기반 게임**에서 일반화 잠재력 보유.  
- **PAST**의 컨텍스트별 일반화($$Q_p(p,a)$$)는 특정 상황에 특화된 성능을 이끌지만, **동적 특성 선택** 기법 연구를 통해 오버헤드를 줄이고 일반화 범위를 넓일 수 있음.  
- **조합 기법**은 각 단계(선택 vs. 플레아웃)의 학습 기법을 분리 설계함으로써 **모듈화된 확장**이 가능하며, 새로운 학습 모듈 추가 시 유연하게 통합 가능.

***

## 5. 향후 연구에 미치는 영향 및 고려 사항  

- **게임 지식 자동 추론 강화**: GDL 분석을 통한 다양한 특징(예: 대각선 이동, 영역 통제) 자동 식별 연구.  
- **동적 파라미터 튜닝**: τ, C, α, λ 등 하이퍼파라미터를 게임별·상황별로 **온라인 적응**시키는 메타러닝 기법 필요.  
- **오버헤드 최소화**: PAST 같은 방법의 효율화 및 **스파스 업데이트** 전략 개발.  
- **통합 프레임워크**: 다양한 시뮬레이션 제어 기법을 **플러그인 형태**로 조합·교체 가능하도록 설계하여 확장성 및 유지보수성 확보.  

이 논문은 GGP 에이전트의 **시뮬레이션 제어 학습**에 관한 기초를 다졌으며, 향후 **게임 일반화**와 **자동화된 파라미터 최적화** 분야로의 연구 확장을 촉발할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/8cfe9f7c-34df-4566-8da9-b1733e4be9d1/7651-13-11181-1-2-20201228.pdf
