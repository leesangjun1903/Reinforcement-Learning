# REINFORCE : Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning

**핵심 주장 및 주요 기여**  
본 논문은 확률적 유닛을 포함하는 연결주의 네트워크에 적용 가능한 일반적인 강화학습 알고리즘 계열인 **REINFORCE 알고리즘**을 제안한다. 이들 알고리즘은 보상을 명시적으로 미분하거나 저장하지 않고도 기대 보상의 기울기 방향으로 가중치를 갱신하며, 즉시 보상 및 일부 지연 보상 과제에서 모두 그레이디언트 상승 효과를 보장한다. 또한, 기존 기법과 자연스럽게 통합 가능한 형태로 제시하여, 백프로파게이션과의 결합 활용 가능성을 보였다.[1]

## 1. 해결하고자 하는 문제  
전통적 강화학습에서 정책(policy) 파라미터를 그레이디언트 방향으로 조정하려면 보상의 기댓값에 대한 기울기를 명시적으로 계산하거나, 이를 추정하기 위한 모델을 학습해야 한다. 이 과정은 복잡하고, 저장/추정 오차에 민감하다.  
논문은  
- **확률적 입출력 매핑**을 수행하는 연결주의 네트워크  
- **즉시 및 제한적 지연 보상** 문제  
에서, 보상의 기댓값 $$\,E\{r\mid W\}$$의 기울기 $$\nabla_W E\{r\mid W\}$$를 직접 계산하지 않고도 적용 가능한 간단·비모델 기반 업데이트 규칙을 제시한다.[1]

## 2. 제안하는 방법  
### 2.1 성능 지표  
강화학습 목표는 파라미터 $$W$$ 아래 기대 보상 $$\,J(W)=E\{r\mid W\}$$을 최대화하는 것이다.  

### 2.2 REINFORCE 업데이트 규칙  
각 가중치 $$w_{ij}$$에 대해, 보상 수령 시  

$$
\Delta w_{ij} \;=\;\alpha_{ij}\,(r - b_{ij})\,\frac{\partial\ln g_i(y_i\mid w_i,x_i)}{\partial w_{ij}}
$$  

를 적용한다.  
- $$\alpha_{ij}$$: 학습률  
- $$b_{ij}$$: 보상 기준선(baseline)  
- $$g_i(\cdot)$$: i번 유닛의 출력 분포(예: Bernoulli-logistic)  
이때 $$\partial\ln g_i/\partial w_{ij}$$를 **특성 적합도(eligibility)**라 부른다.[1]

### 2.3 기울기 일치 보장  
정리 1. REINFORCE 알고리즘은 평균 업데이트 벡터 $$E\{\Delta W\mid W\}$$가 $$\nabla_W E\{r\mid W\}$$와 비음수 내적을 가지며, 일정 학습률일 때 정확히 그 비례 관계를 가진다.[1]
즉,  

$$
E\{\Delta W\mid W\}\;\propto\;\nabla_W E\{r\mid W\},
$$  

따라서 평균적으로 기대 보상을 증가시키는 방향으로 학습이 진행된다.

## 3. 모델 구조  
- **네트워크**: 피드포워드 연결주의 네트워크  
- **유닛**: 입력 합산 후 로지스틱 함수를 거친 확률적 출력(Bernoulli-logistic)  
- **탐색**: 출력의 확률적 선택을 통해 구현  
- **연결**: 백프로파게이션과 통합 가능  

## 4. 성능 향상 및 한계  
### 성능  
- 보상 기반 정책 학습에서 **단순 구현**에도 경쟁력 있는 성능을 보였으며, 백프로파게이션과 결합 시 복잡 과제에도 확장 가능.[1]
- **분산 감소**를 위한 기준선(baseline) 사용으로 학습 안정성 제고가 가능하다.  

### 한계  
- 전통적 그레이디언트 기법과 마찬가지로 **국소 최적해(local optima)**에 빠질 위험이 있고, **수렴 이론**이 일반적으로 부재하다.  
- **지연 보상** 과제 전반에 걸친 성능 보장은 추가 기법(critic, TD 학습 등) 필요.[1]
- 학습률 및 기준선 설정에 민감하며, **분산**이 큰 편이어서 샘플 효율성이 떨어질 수 있다.

## 5. 일반화 성능 개선 가능성  
- **기준선(baseline) 최적화**: 보상 분산을 줄여 기울기 추정의 노이즈를 낮추면 더 나은 일반화 성능이 기대된다.  
- **모델 기반 요소 결합**: 단위별 국소 모델이나 전역 모델(backprop through model)을 결합하면 기울기 추정 정확도가 향상될 수 있다[8.5절].  
- **에피소드 형태 확장**: 전체 에피소드 보상을 이용하는 Episodic REINFORCE로 일반화 강화를 도모할 수 있다.  

## 6. 향후 연구에 미치는 영향 및 고려사항  
- **그레이디언트 기반 강화학습 발전의 기초**: 이후 배우-비평가(Actor–Critic), 정책 경사 정책(policy gradient) 알고리즘의 이론적 토대가 되었다.  
- **수렴 이론** 확보: 전반적 안정성과 수렴 조건에 대한 엄밀한 분석이 요구된다.  
- **지연 보상 과제 통합**: Temporal Difference, 크리틱 네트워크 등과의 통합 방안 연구가 필요하다.  
- **분산 감소 기법 연구**: 기준선 자동 학습, 중요도 샘플링, 자연정책경사(Natural Policy Gradient) 등 분산 완화 메커니즘을 고찰해야 한다.  

이 논문은 **정책 경사 기반 강화학습** 분야의 시발점으로, 간단하면서도 확장 가능한 알고리즘 설계 원칙을 제시했다는 점에서 큰 의의를 가진다. 미래 연구에서는 **수렴성 보장**, **지연 보상 처리**, **분산 감소** 측면에 주목해 더욱 강력한 알고리즘을 개발해야 할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ff94488-66ce-4aac-bd0c-7c37f153901a/BF00992696.pdf)
