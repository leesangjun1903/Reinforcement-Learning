# Neural Architecture Search with Reinforcement Learning | NAS

## 1. 핵심 주장 및 주요 기여  
**Neural Architecture Search (NAS)**는 강화학습을 이용하여 신경망 구조 자체를 자동으로 탐색·설계하는 메커니즘을 제안한다.  
- **주요 기여**  
  1. RNN 기반 컨트롤러가 가변 길이의 아키텍처 문자열을 생성하고, 강화학습(REINFORCE)을 통해 검증 정확도 보상을 최대화하도록 학습  
  2. CIFAR-10 이미지 분류와 Penn Treebank 언어 모델링에서 최첨단 이상의 성능 달성  
  3. 스킵 연결·풀링·배치 정규화 등 현대적 모듈을 포함하는 확장된 탐색 공간 설계  
  4. 병렬 분산 학습을 통해 수천 개의 후보 모델을 효율적으로 평가  

## 2. 문제 정의 및 제안 방법  
### 2.1 해결하고자 하는 문제  
- 기존 CNN·RNN 아키텍처는 전문가의 수작업 탐색이 필요하며, 탐색 공간이 고정 길이·수동 설계에 제한됨  
- 자동화된, 가변 길이 구조 탐색을 통해 인간 설계를 뛰어넘는 모델을 찾고자 함  

### 2.2 NAS 프레임워크  
1) **컨트롤러 RNN**  
   - LSTM 기반으로 각 층의 하이퍼파라미터(필터 크기, 필터 수, 스킵 연결, 레이어 타입 등)를 토큰 시퀀스로 예측  
   - 가변 길이 시퀀스 생성  

2) **강화학습 업데이트**  
   - 아키텍처 a₁:ₜ의 검증 정확도 R을 보상으로 사용  
   - 목표: $$J(\theta_c)=\mathbb{E}\_{P(a_{1:T};\theta_c)}[R]$$ 최대화  
   - REINFORCE 식:  

$$
       \nabla_{\theta_c}J=\sum_{t=1}^T \mathbb{E}\big[\nabla_{\theta_c}\log P(a_t\mid a_{1:t-1};\theta_c)\,R\big]
     $$  

  - 보상 분산 감소를 위해 지수이동평균 기반 baseline $$b$$ 활용  

3) **확장된 탐색 공간**  
   - **스킵 연결**: 콘텐츠 기반 시그모이드 집합 선택(attention)으로 임의 층 연결  
   - **레이어 타입**: 풀링·배치정규화·학습률 예측 추가 가능  
   - **재귀 셀 설계**: 트리 구조로 RNN 셀 연산 노드(label, 합·곱·활성화)를 예측  

4) **병렬·분산 평가**  
   - 수십∼수백 개의 컨트롤러 복제본과 수천 개의 GPU/CPU에서 동시 평가  
   - 매 배치마다 m개 후보 모델 훈련 후 gradients 집계·업데이트  

### 2.3 모델 구조 및 수식  
- 컨트롤러: 2-layer LSTM, hidden=35, ADAM(η=6e−4)  
- child 네트워크: CIFAR-10용 CNN(최대 39층), PTB용 RNN 셀(base=8)  
- 보상 함수: CIFAR-10 → $$(\max_{최근5epoch}\text{val acc})^3$$, PTB → $$\bigl(c/\text{val perplexity}\bigr)^2$$  

## 3. 성능 향상 및 한계  
### 3.1 성능 향상  
- **CIFAR-10**  
  - 3.65% 오류율, DenseNet-BC(27.2M) 대비 0.09% 개선·1.05× 속도↑  
- **Penn Treebank**  
  - 신규 RNN 셀 test perplexity 62.4 → 종전 최저 대비 −3.6 개선  
  - 문자 언어 모델에서 bits-per-char 1.214 (state-of-art)  

### 3.2 한계  
- 막대한 컴퓨팅 리소스(수백 GPU/CPU) 필요  
- 탐색 공간 확장 시 sample 수 제한으로 지역 최적 가능성  
- NAS로 찾은 모델 해석·일반화 메커니즘 불투명성  

## 4. 일반화 성능 향상 가능성  
- **전이 학습 사례**: PTB에서 찾은 셀을 문자 모델과 기계번역(GNMT)에 그대로 적용  
  - 문자 모델 BPC 개선, GNMT BLEU +0.5  
- **보편적 셀 구조** 발견 → 다양한 NLP/CV 태스크에 적용 가능성 제시  
- **탐색 공간 유연성** 덕에 새로운 활성화·합성 연산 추가도 가능  

## 5. 향후 연구 방향 및 고려점  
1. **효율적 탐색**: 강화학습 대신 진화 알고리즘·베이지안 최적화 결합으로 자원 절감  
2. **해석 가능성 강화**: 찾은 아키텍처·셀의 내부 동작 메커니즘 분석  
3. **경량화 모델**: 모바일/엣지 환경 적합한 저비용 NAS 기법 개발  
4. **도메인 특화 NAS**: AutoML과 결합해 의료·과학 분야 특화 구조 자동생성  
5. **공정성·안전성**: NAS로 탐색된 모델의 편향·안정성 보장 기법 통합  

---  
**주요 시사점**: NAS는 인간 설계를 뛰어넘는 신경망 구조를 자동으로 탐색·발견하며, 대규모 병렬 리소스만 확보된다면 CV·NLP 전 분야에 적용 가능한 혁신적 프레임워크이다. 앞으로는 효율성·해석성·안정성에 초점을 맞춘 NAS 개선이 필수적이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/71d8ab93-d9ba-4bd1-b8f1-d16989b45d50/1611.01578v2.pdf
