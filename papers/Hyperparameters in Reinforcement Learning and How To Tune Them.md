# Hyperparameters in Reinforcement Learning and How To Tune Them

## 1. 핵심 주장 및 주요 기여  
본 논문은 강화학습(RL) 알고리즘에서 하이퍼파라미터가 성능과 재현성에 미치는 영향을 체계적으로 분석하고, AutoML 분야의 최첨단 하이퍼파라미터 최적화(HPO) 기법을 도입함으로써 다음과 같은 기여를 제시한다.  
- RL에서 다양한 하이퍼파라미터가 성능과 샘플 효율에 결정적 영향을 미치며, 튜닝 시드(seed)에 따라 최적 파라미터가 크게 달라져 오버피팅 가능성이 큼을 실증.  
- 랜덤 탐색, DEHB(진화+멀티피델리티), PB2(베이지안 PBT) 등 여러 HPO 도구가 수작업 그리드 탐색 대비 계산비용을 대폭 줄이면서 더 나은 성능을 달성함을 보임.  
- AutoML의 모범 사례(튜닝·테스트 시드 분리, 멀티피델리티, 광역 탐색) 도입을 위한 구체적 가이드라인과 플러그앤플레이 구현체 제공.

## 2. 문제 정의 및 제안 방법  
### 2.1 문제 정의  
강화학습 알고리즘 $$A$$의 하이퍼파라미터 공간 $$\Lambda$$에서 환경 분포 $$\mathcal{I}$$에 걸친 기대비용(또는 음수 보상)  

$$
\lambda^* = \arg\min_{\lambda\in\Lambda}\mathbb{E}_{i\sim\mathcal{I}}[c(A(i;\lambda))]
$$  

를 최소화하는 튜닝 과정을 다룬다. 튜닝 시드와 테스트 시드를 분리하지 않으면 튜닝 시드 오버피팅이 발생한다.

### 2.2 제안 방법  
1. **기존 HPO 도구 비교**  
   - Random Search (RS)  
   - DEHB (HyperBand + Differential Evolution)  
   - PB2 (Bayesian 기반 Population-Based Training)  
2. **실험적 검증**  
   - 다양한 환경(Pendulum, Ant, MiniGrid 등)·알고리즘(PPO, DQN, SAC)에서 7–11개 하이퍼파라미터 전수 탐색  
   - 예산 10–64 전체 실행(run) 내에서 각 HPO 도구 성능 비교  
3. **Best Practice 제안**  
   - 튜닝과 테스트 시드 분리  
   - 멀티피델리티 평가 및 광범위 탐색  
   - 동일 예산·하드웨어·프로토콜로 제안 기법과 비교 대조  
4. **수식**  
   - **Dynamic Algorithm Configuration (DAC)**  
     
$$
     \pi^* = \arg\min_{\pi\in\Pi} \mathbb{E}_{i\sim\mathcal{I}}[c(A(i;\pi(s,i)))]
     $$  
     
상태 $$s$$별 동적 하이퍼파라미터 정책 $$\pi$$ 학습을 강조.

## 3. 모델 구조 및 성능 향상  
- **튜닝 예산 10 runs**: DEHB가 사전 수작 튜닝 대비 평균 1.5×–2× 보상 향상, RS·PB2도 전통 그리드 대비 우수.  
- **소규모 예산(16 runs)**: DEHB가 Brax·Procgen 환경에서 최고 순위 달성.  
- **대규모 예산(64 runs)**: DEHB 성능 추가 향상, PB2·BGT(진화형 PBT)는 오버피팅 경향, RS 성능 정체.  
- **계산 비용**: 모든 기법이 전체 러닝 타임 대비 오버헤드 미미(최대 2시간 미만), 수작업 대비 수십 배 저렴.

## 4. 일반화 성능 향상 관점  
- 튜닝·테스트 시드 분리 실험에서, 단일 시드 튜닝 시 Incumbent 대비 Test 세드 성능이 최대 4배 저하됨을 확인.  
- 3–5개 튜닝 시드 사용 시 일반화 성능이 상승하나, 시드가 10개 넘어가면 HPO 문제 난이도 급증으로 Test 성능 저하.  
- DEHB는 멀티피델리티로 저예산 단계에서도 다양한 구성을 살펴봄으로써 오버피팅을 줄이고 일반화를 확보.

## 5. 한계 및 고려사항  
- **시드 민감도**: RL 특유의 랜덤성으로 인해 동일 구성에서도 시드별 편차 큼.  
- **동적 튜닝 기법 부재**: PBT 계열이 정적 구성 유지에 그쳐 DAC 패러다임 완전 활용 미흡.  
- **하이퍼파라미터 상호작용**: 간헐적 상호작용이 존재하나 대부분 독립적이며, 복잡한 상호작용 탐색에 추가 연구 필요.

## 6. 향후 연구 영향 및 고려점  
- **AutoRL 생태계 확장**: 본 논문의 구현체(Hydra 플러그인)와 가이드라인이 RL 라이브러리(HuggingFace, Stable-Baselines 등)에 통합되어야 함.  
- **동적 정책 학습**: DAC·메타그래디언트 기반 온더플라이 튜닝 연구 활성화 가능.  
- **벤치마크 발전**: AutoRL-Bench 등 메타튜닝 벤치마크를 통한 도구 간 공정 비교 촉진.  
- **재현성 제고**: 논문 발표 시 튜닝 프로토콜·시드·구성공간을 완전 공개하는 표준 수립.

***
위 권고사항을 따르면 RL 연구의 **재현성**, **공정 비교**, **샘플 효율성**이 크게 개선되어, 향후 새로운 환경·알고리즘 개발 시 성능 평가의 신뢰도를 높일 수 있을 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/33b4d0e6-e983-4a16-a076-dd8cdfd906f6/2306.01324v1.pdf
