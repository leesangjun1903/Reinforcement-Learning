# A Distributional Perspective on Reinforcement Learning

**핵심 주장 및 주요 기여**  
본 논문은 기존 강화학습이 *기댓값*만을 학습하는 데 주목한 것과 달리, 에이전트가 받는 **무작위 누적 보상의 분포**(value distribution)를 직접 모델링할 때 학습 안정성과 성능이 크게 향상됨을 주장한다. 주요 기여는 다음과 같다:[1]

- 정책 평가(policy evaluation)에서 분포형 벨만 연산자의 **Wasserstein 거리** 내 수렴성 증명  
- 제어(control) 환경에서 분포형 최적화 연산자의 **불안정성** 분석  
- 분포형 벨만 최적성 방정식을 기반으로 한 **Categorical DQN** 알고리즘 제안  
- Arcade Learning Environment(Atari 2600)에서 **state-of-the-art** 성능 획득  

***

## 1. 해결하고자 하는 문제  
일반적인 강화학습은 액션-가치 함수 $$Q(x,a)=\mathbb{E}[Z(x,a)]$$의 기댓값만을 학습하므로, 다음과 같은 문제점이 존재한다:

- **비선형 근사** 상황에서 그리디 업데이트에 의한 **불안정성 및 발산**  
- 환경의 **불확실성** 또는 **부분 관측**으로 인한 불안정 학습  
- 희소 보상(sparse reward) 전파의 어려움  

***

## 2. 제안 방법

### 2.1 분포형 벨만 방정식  
반환값 분포 $$Z(x,a)$$는 다음과 같은 **분포형 벨만 방정식**을 만족한다:[1]

$$
Z(x,a)\;\overset{D}{=}\;R(x,a)\;+\;\gamma\,Z(X',A')
$$

여기서 $$R$$은 보상, $$\gamma$$는 할인율, $$(X',A')$$는 다음 상태-행동이다.

### 2.2 Wasserstein 거리 수렴성  
정책 평가 연산자 $$\mathcal{T}$$에 대해, 임의의 두 분포 $$Z_1,Z_2$$ 및 Wasserstein $$p$$-거리 $$d_p$$에 대하여  

$$
d_p\bigl(\mathcal{T}Z_1,\;\mathcal{T}Z_2\bigr)\;\le\;\gamma\,d_p(Z_1,Z_2)
$$  

임을 증명하여 **수렴성**을 보였다.[1]

### 2.3 Categorical DQN 알고리즘  
분포를 유한 개의 **atom** $$\{z_i\}_{i=0}^{N-1}$$으로 근사하고, 각 atom의 확률을 신경망으로 출력한다.  
1. 샘플링 전이 $$(x,a,r,x')$$에 대해 분포형 Bellman 타겟을 계산  
2. **Wasserstein-projection** 대신 **cross-entropy** 손실로 다중 클래스 분류 문제로 변환  
3. DQN 구조에 통합하여 **Categorical DQN (C51)** 학습  

***

## 3. 모델 구조  
- **입력**: Atari 프레임 4장 스택  
- **공통 CNN 블록** → **fully connected 레이어** → **$$N$$개의 atom 확률** 출력  
- 정책은 기대값 $$\mathbb{E}[Z]$$에 기반한 $$\epsilon$$-greedy 사용  

***

## 4. 성능 향상 및 한계

### 성능 향상  
- 57개 Atari 게임 평균 성능에서 기존 DQN 대비 **126%**, median **21.5%** 향상.[1]
- **Sparse reward** 환경에서도 빠른 보상 전파로 **VENTURE, PRIVATE EYE** 등에서 월등한 성능  
- 50M 프레임 학습만으로도 완전 학습된 DQN(200M)보다 우수  

### 한계  
- 최적성 연산자의 **비수축성**으로 인한 이론적 불안정성 존재[1]
- Atom 개수 및 지원 범위($$V_{\min},V_{\max}$$) 하이퍼파라미터 의존  
- Continuous action/reward 환경으로의 일반화 미검증  

***

## 5. 일반화 성능 향상 가능성  
분포학습은 근사오차로 인한 그리디 정책의 **chattering**을 완화하고, **부분 관측**·**내재적 불확실성**을 직접 모델링하여 안정적 학습을 유도한다. 또한 multi-modal 분포 예측은 희소 보상 환경에서 **희귀하지만 중요한 이벤트** 전파를 돕는다.[1]

***

## 6. 영향 및 향후 연구 고려사항  
본 연구는 강화학습 이론과 실무 모두에 분포 관점의 중요성을 강조하며, 이후 연구 시 다음을 고려해야 한다:

- **연속 공간**에서의 분포 근사 방식 (e.g., normalizing flow)  
- **확률적 정책** 및 **보수적 업데이트**와의 결합으로 제어 안정성 강화  
- **Wasserstein 손실**을 직접 최적화할 수 있는 효율적 방법론 개발  
- 실제 로보틱스·자율주행 등 **연속 제어 문제**로의 확장  

이러한 방향은 분포학습의 이점을 더욱 부각시키고, 강화학습의 안정성과 일반화 성능을 획기적으로 개선할 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/cb340d3a-3138-4285-8d8d-c6ff77b27e78/1707.06887v1.pdf)
