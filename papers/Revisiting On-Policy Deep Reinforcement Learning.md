# Revisiting On-Policy Deep Reinforcement Learning

**주요 주장 및 기여 요약**  
이 논문은 기존 온-폴리시(on-policy) 강화학습 알고리즘들이 이론적 정책 그래디언트를 왜곡된 형태로 사용하고, 과도하게 복잡한 신뢰 영역(trust‐region) 및 모멘텀 기반 최적화에 의존한다는 문제를 지적한다. 제안하는 ON-SAC(On-Policy Soft Actor-Critic)는  
1. **진정한 온-폴리시(policy) 그래디언트**를 사용하기 위해 할인된 상태분포 $$d^\pi_\gamma$$를 도입하고,  
2. **모멘텀 없는 순수 그래디언트 상승(pure gradient ascent)** 기법(RMSProp)을 적용하며,  
3. **오프-폴리시(off-policy) 데이터를 활용한 비평가(critic) 학습**으로 표본 효율성과 안정성을 높인다.  
이로써 PPO보다 간결하면서도 6개 MuJoCo 환경 중 4곳에서 우수한 성능을 보이며, 온-폴리시 강화학습의 새로운 기준을 제시한다.

***

## 1. 해결하고자 하는 문제  
- **편향된 그래디언트 추정**: 대부분 온-폴리시 알고리즘은 $$\nabla_\phi J(\phi) = \mathbb{E}\_{s\sim d^\pi_\gamma,a\sim\pi} [Q^\pi(s,a)\nabla_\phi \log\pi_\phi(a|s)]$$이론 대신 상태 샘플 평균을 사용해, 최적화 대상 함수과 일치하지 않는 편향된 그래디언트를 계산한다.  
- **복잡성 및 과감한 튜닝**: PPO나 TRPO는 신뢰 영역, 여러 손실함수, 코드 수준 최적화가 필요해 하이퍼파라미터에 민감하다.  
- **모멘텀 기반 옵티마이저의 편향 문제**: Adam 등의 모멘텀은 지난 그래디언트 정보가 현재 정책 업데이트에 과도하게 반영되어 “온-폴리시” 특성을 약화시킨다.

***

## 2. 제안하는 방법: ON-SAC  
### 2.1 수식 및 구성  
1) **진정한 할인 정책 그래디언트**  

$$
     \nabla_\phi J(\phi)
     = \frac{1}{1-\gamma}\,\mathbb{E}\_{s\sim d^\pi_\gamma,a\sim\pi_\phi}
       \bigl[Q^\pi(s,a)\,\nabla_\phi\log\pi_\phi(a|s)\bigr]
   $$  
   
   - 이론대로 $$\gamma$$-가중 상태분포 $$d^\pi_\gamma$$를 직접 반영하여 편향을 제거.  
2) **모멘텀 제거 및 RMSProp**  
   - Adam의 이동 평균(moving average) 대신 RMSProp 최적화로 순수 그래디언트 상승을 수행.  
3) **오프-폴리시 비평가 학습**  
   - 기존 SAC처럼 리플레이 버퍼 $$B$$에서 과거 전이 $$(s,a,r,s')$$를 사용하여 TD 오차  

     $$\delta_t = r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)$$  

     에 기반해 critic 파라미터를 갱신.  

### 2.2 모델 구조  
- **액터(actor)**: 연속 동작 공간에서 확률적 정책 $$\pi_\phi(a|s)$$, 네트워크 레이어 2개, 유닛 256개, 비선형성 TanH  
- **비평가(critic)**: 앙상블 5개 $$\{Q_{\theta_i}\}_{i=1}^5$$, 동일한 구조  
- **학습 스케줄**:  
  - 매 정책 업데이트마다 5 에피소드 수집  
  - 감가율 $$\gamma=0.995$$, 생존 보상(survival reward) 절반 감소  
  - 배치 크기 256, 학습률 $$3\times10^{-4}$$

***

## 3. 성능 향상 및 한계  
- **성능 비교**: MuJoCo-v5 6개 환경 중 Hopper, Walker2d, HalfCheetah, Ant에서 PPO 대비 월등한 샘플 효율성 및 최종 성능 달성. InvertedDoublePendulum와 Humanoid는 유사하거나 소폭 열세.  
- **설계 선택 영향**  
  - 모멘텀 추가 시 성능 전반 악화(인버티드 이중 진자 제외)  
  - 오프-폴리시 학습이 비평가 성능 향상, 온-폴리시만 사용 시 Humanoid 학습 실패  
  - 할인 그래디언트 적용은 HalfCheetah에서 유의미 개선  
- **한계**  
  - 높은 차원 Humanoid 환경에서 불안정성 여전  
  - 하이퍼파라미터(리플레이 버퍼 크기, critic 수) 민감성  
  - 비평가 앙상블 필요로 인한 계산 비용 증가

***

## 4. 모델의 일반화 성능 향상 가능성  
- **진정한 온-폴리시 그래디언트**: 이론적 편향 제거로 다양한 환경에 걸쳐 안정적 일반화 기대  
- **오프-폴리시 데이터 활용**: 더 다양한 상태분포 학습 → 과적합 감소 및 강인성 향상  
- **모멘텀 제거**: 비선형 비평가 추정치 편향에 따른 불안정성 완화로 일반화 성능 개선 가능  
- **확장 방향**:  
  - 앙상블 규모 및 재샘플링 전략 자동화  
  - 적응적 감가율 조정 기법 도입  
  - 메타-튜닝 없는 하이퍼파라미터 로버스트화 연구  

***

## 5. 향후 연구에 미치는 영향 및 고려사항  
- **이론적 준거로서 기준 제시**: 온-폴리시 강화학습의 근본적 설계 원칙(할인 그래디언트·모멘텀 배제) 강조  
- **간결성 강조**: 복잡한 신뢰 영역·코드 최적화 없이도 고성능 달성 가능성 시사  
- **고려사항**:  
  1. **고차원 제어 과제**에서의 안정성 보장 메커니즘 연구  
  2. **하이퍼파라미터 자동화**: 앙상블 수·버퍼 크기 무감도 설계  
  3. **다중 태스크 일반화**: 온-폴리시 SAC 방식을 Meta-RL 및 분산 RL에 확장  
  4. **이론적 해석**: 할인된 정책 그래디언트 수렴 특성 분석  

ON-SAC는 온-폴리시 강화학습 연구에 **이론적 일관성**과 **실용적 간결성**을 모두 제시하며, 향후 **안정적·효율적 일반화**를 지향하는 알고리즘 설계의 토대를 제공할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/251f5dd4-0de2-4a7d-9c13-5df9054cff38/118_Revisiting_On_Policy_Deep.pdf
