# PER : Prioritized Experience Replay

**핵심 주장 및 기여 요약**  
Prioritized Experience Replay(PER)는 강화학습 에이전트가 과거 경험을 균등하게 재생하는 대신, **Temporal-Difference(TD) 오류**가 큰 중요한 전이(transition)를 우선 재생하여 학습 효율을 크게 개선하는 방법을 제안한다. 이를 통해 DQN 기반 에이전트가 49개 Atari 게임 중 41개에서 균등 재생 대비 성능 향상을 이루었으며, 학습 속도가 평균 두 배 이상 빨라졌다.[1]

***

## 1. 해결하고자 하는 문제  
전통적 경험 재생(Experience Replay)은 메모리에 저장된 전이를 균등 샘플링하여 학습하지만,  
- 상관관계가 높은 연속 전이의 반복  
- 희귀하지만 중요한 경험의 빠른 소실  
문제를 일으킨다.[1]

***

## 2. 제안 방법  
### 2.1 우선순위 기준  
- **TD 오류** $$\delta_i = r_i + \gamma \max_a Q(s_{i+1},a) - Q(s_i,a_i)$$의 절댓값 $$|\delta_i|$$를 전이의 우선순위 $$p_i$$로 사용[1].  
- 새 전이는 최대 우선순위로 초기화하여 최소 한 번은 학습에 사용되도록 보장.

### 2.2 확률적 우선순위 재생  
- 전이 $$i$$가 샘플링될 확률  

$$
    P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha},
  $$  
  
  여기서 $$\alpha$$는 우선순위 강도 조절 파라미터($$\alpha=0$$일 때 균등 샘플링).[1]
- **비례 우선순위**: $$p_i = |\delta_i| + \varepsilon$$.  
- **순위 기반 우선순위**: $$p_i = 1/\text{rank}(i)$$.  

### 2.3 중요도 샘플링 보정  
- 편향된 분포를 보정하기 위해 중요도 샘플링 가중치  

$$
    w_i = \bigl(N \cdot P(i)\bigr)^{-\beta}
  $$  
  
  $$\beta$$는 보정 강도, 학습 후반에 1로 선형 증가시켜 편향 제거.[1]
- 업데이트 시 $$\delta_i$$ 대신 $$w_i\delta_i$$를 사용하여 수렴점을 유지.

***

## 3. 모델 구조  
- **Double DQN** 기반 구조에 PER 모듈만 교체하여 적용.[1]
- 입력: 상태 $$s$$ (Atari 프레임)  
- 네트워크: 3개 컨볼루션 + 2개 fully-connected 층  
- 출력: 각 행동의 Q값  
- 경험 재생 메모리 크기 $$N=10^6$$, 미니배치 크기 32, 클리핑 보상 및 TD 오류 $$\in[-1,1]$$.  

***

## 4. 성능 향상 및 한계  
### 4.1 성능 향상  
- **학습 속도**: 평균 두 배 이상 빨라짐.  
- **최종 성능**: Double DQN 대비 57개 게임에서 중간 성능(median) 111→128, 평균(mean) 418→551 상승.[1]
- 41/49 게임에서 균등 재생 대비 점수 향상 관측.[1]

### 4.2 한계 및 고려사항  
- **다양성 부족**: 높은 오류 전이에 치우쳐 과적합 위험  
  → 순위 기반, 확률적 샘플링으로 완화  
- **하이퍼파라미터($$\alpha,\beta$$) 조정 필요**  
- **메모리·연산 오버헤드**: $$O(\log N)$$ 구조(합 트리, 이진 힙) 구현으로 경감  
- **노이즈 민감성**: 보강 샘플링 및 보정 필요.[1]

***

## 5. 일반화 성능 향상 관점  
- **희귀 전이 보존**: 중요하나 드문 경험을 반복 학습 → 강화된 희귀 상황 일반화  
- **표현 학습 상호작용**: 오류가 줄어든 전이는 재생 빈도 감소 → 표현 개선이 필요한 전이에 집중하여 기능 표현 능력 향상  
- **바이어스-분산 균형**: $$\beta$$ 보정으로 학습 편향 완화 → 최종 정책 일반화 보장

***

## 6. 향후 연구 영향 및 고려사항  
- **강화학습 외 응용**: 불균형 슈퍼바이즈드·오프폴리시 RL에 우선순위 샘플링 확장 가능  
- **메모리 관리**: 우선순위 기반 메모리 유지·삭제 전략으로 효율 향상  
- **탐색 보조 신호**: 재생 횟수 $$M_i$$를 탐색 하이퍼파라미터 적응에 활용  
- **하이브리드 우선순위**: TD 오류 외 보상 수익, 사후 학습 변화량 등 복합 기준 실험  
- **하이퍼파라미터 자동화**: $$\alpha,\beta$$ 적응적 조정으로 튜닝 부담 감소

Prioritized Experience Replay는 **효율적이고 강건한 경험 재생 기법**으로 강화학습 성능과 학습 속도를 크게 향상시켰으며, 다양한 변형과 응용 가능성이 기대된다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3521d02e-6f0b-44de-94a7-916b4e9493cd/1511.05952v4.pdf)
