# AlphaGo : Mastering the Game of Go with Deep Neural Networks and Tree Search

**핵심 주장 및 주요 기여**  
AlphaGo 논문은 인간 전문가의 기보로 학습한 **딥 컨볼루션 신경망**(정책 네트워크)과 **강화학습** 절차로 최적화된 정책 네트워크, 그리고 **가치 네트워크**를 결합한 **Monte-Carlo Tree Search**(MCTS) 알고리즘을 제안한다. 이 방법으로 구(Go) 게임에서 기존 최첨단 MCTS 기반 프로그램들을 능가하며, 첫 프로 대국 승리를 달성했다.

***

## 1. 문제 정의  
Go는 높은 분기(b≈250)와 깊이(d≈150)를 가져 완전 탐색이 불가능한 결정적 완전 정보 게임이다.  
- 최적 가치함수 $$v^*(s)$$를 정확히 계산하기 위해서는 지수적 크기의 트리 탐색 필요  
- 기존 α-β 탐색이나 전통적 MCTS는 포지션 평가와 행동 선택의 한계 존재  

## 2. 제안 방법  
### 2.1 신경망 구성 요소  
- **SL 정책 네트워크** $$p_\sigma(a|s)$$  
  - 입력: 19×19×48 특징 맵  
  - 13개 합성곱 레이어, 마지막에 소프트맥스  
  - 지도학습으로 인간 기보 2900만 포지션 학습  
- **RL 정책 네트워크** $$p_\rho(a|s)$$  
  - $$p_\sigma$$ 초기화 후 self-play 기반 정책 그래디언트 강화학습  
  - 목표: 승률 최대화  
- **가치 네트워크** $$v_\theta(s)$$  
  - 입력: SL 특징 + 현재 플레이어 색  
  - 14개 레이어, 마지막 tanh 출력  
  - self-play로 생성한 3000만 비상관 샘플 포지션에 대해 회귀 학습  

### 2.2 MCTS 통합  
각 시뮬레이션에서  
1. **선택**: PUCT 식  

$$
     a_t = \arg\max_a \bigl[Q(s,a) + c_{\text{puct}}P(s,a)\frac{\sqrt{\sum_b N(s,b)}}{1+N(s,a)}\bigr]
   $$  

2. **확장**: 방문 횟수 임계치 초과 시 노드 생성, 초기 prior $$P(s,a)=p_\sigma(a|s)$$  
3. **평가**:  
   - 가치 네트워크 $$v_\theta(s_L)$$  
   - 롤아웃 $$z_L$$  
   - 혼합 평가  

$$
       V(s_L) = (1-\lambda)v_\theta(s_L) + \lambda z_L,\quad \lambda=0.5
     $$  

4. **백업**: Q, N 통계 업데이트  

### 2.3 구현 및 하드웨어  
- CPU 48코어, GPU 8개 단일 머신으로 비동기 MCTS  
- 분산 버전: CPU 1202코어, GPU 176개  
- 5초 제한, 40스레드, 비대칭 평가 큐  

## 3. 성능 향상  
- 전통적 MCTS 대비 가치 네트워크만으로도 우수한 성능  
- 최종 AlphaGo(분산) Elo 3140, 인간 프로 2단(Fan Hui, Elo 2908) 5–0 완승  
- 가치 네트워크 도입 시 탐색 효율 15,000배 절감  

## 4. 한계  
- 학습 데이터: 전문 기보 의존 → 소수 기보 도메인 일반화 어려움  
- 매우 높은 연산 자원 요구  
- 복잡한 하이퍼파라미터(λ, $$c_{puct}$$, 임계치 등) 튜닝 필요  

***

## 5. 모델의 일반화 성능 향상 가능성  
- **self-play 데이터 다양화**: 다양한 초기정책 혼합으로 포지션 분포 확장  
- **도메인 무관적 특징**: 기보 외 환경 특징 추가→다른 보드 게임 전이 학습  
- **경량화 네트워크**: 지식 증류로 작고 빠른 평가기 도출 → 실시간 적용성 개선  
- **데이터 효율성**: 소수 샘플 학습 극대화 위해 메타학습·비지도 사전학습  

***

## 6. 향후 연구에 미치는 영향 및 고려사항  
AlphaGo는 **딥 RL**과 **트리 탐색** 결합의 가능성을 입증해, 다양한 **결정적 게임**, **계획 문제**, **제약 충족** 등으로 확대 적용되었다.  
- **확장성 연구**: 적은 연산 자원으로 유사 성능 달성 방법  
- **안전·해석성**: 신경망 의사결정 근거 탐색  
- **멀티 에이전트**: 상호작용 환경에서 정책·가치 평가 통합  
- **일반 지능**: Go 외 복잡한 실제 문제(로봇 제어, 물류 계획)에의 전이  

연구 시 **데이터 다양성**, **자원 효율성**, **해석 가능성**을 균형 있게 고려해야 한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/61589155-32fb-4ce4-b335-497af5ecc078/nature16961.pdf)
