# PPO : Proximal Policy Optimization Algorithms

**핵심 주장 및 주요 기여**  
Proximal Policy Optimization(PPO)는 신뢰영역 정책 최적화(TRPO)의 **안정성과 신뢰성**을 유지하면서도 **단순성, 범용성, 데이터 효율성**을 크게 개선한 새로운 정책 경사(policy gradient) 계열 알고리즘이다. PPO는 기존 한 번의 배치당 단일 업데이트 방식 대신, **클리핑 기반의 대리 목적함수(surrogate objective)**를 도입해 여러 차례 미니배치 업데이트를 수행함으로써 **샘플 복잡도 감소**, **코드 변경 최소화**, **병렬 확장성** 등을 동시에 달성한다.[1]

***

## 1. 해결하려는 문제  
기존 강화학습 방식들은 다음과 같은 한계를 지녔다.  
- **Q-러닝(딥 Q-러닝)**: 연속 동작공간에서 성능이 불안정하며 이론적 이해 부족  
- **바닐라 정책 경사**: 데이터 효율성 및 과도한 파라미터 업데이트로 인한 불안정성  
- **TRPO**: 안정성은 우수하나 계산 복잡도가 높고 하이퍼파라미터 튜닝이 까다로움[1]

이 논문은 대규모 모델 및 병렬 환경에서도 **단계별 제약(클리핑)**만으로 TRPO급 성능을 내면서 구현 복잡도를 크게 낮추는 방안을 제시한다.[1]

***

## 2. 제안하는 방법  

### 2.1 기본 배경  
- 정책 경사 추정:  

$$
    \hat{g} = \hat{\mathbb{E}}_t\bigl[\nabla_\theta \log \pi_\theta(a_t|s_t)\,\hat A_t\bigr]
  $$  
  
  여기서 $$\hat A_t$$는 어드밴티지(advantage) 추정치이다.[1]

- TRPO 대리 목적함수:  

$$
    L^{\text{CPI}}(\theta)
    =
    \hat{\mathbb{E}}_t\bigl[r_t(\theta)\,\hat A_t\bigr],
    \quad
    r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\mathrm{old}}}(a_t|s_t)}
  $$
  
  제약: $$\hat{\mathbb{E}}\_t[\mathrm{KL}[\pi_{\theta_{\mathrm{old}}},\pi_\theta]]\le\delta$$.[1]

### 2.2 클리핑 기반 대리 목적함수  
- **클리핑된 확률 비율**을 도입해 $$[1-\varepsilon,1+\varepsilon]$$ 밖의 업데이트에 페널티를 부여  
- **최종 목적함수**:  

$$
    L^{\mathrm{CLIP}}(\theta)
    =
    \hat{\mathbb{E}}_t\Bigl[\min\bigl(r_t(\theta)\,\hat A_t,\;\mathrm{clip}(r_t(\theta),1-\varepsilon,1+\varepsilon)\,\hat A_t\bigr)\Bigr]
  $$
  
  이로써 $$\theta_{\mathrm{old}}$$ 인근에서는 원래 대리 목적과 1차 정합을 유지하되,$$\varepsilon$$ 범위를 벗어날 때는 하한을 적용해 과도한 업데이트를 방지한다.[1]

### 2.3 전체 학습 알고리즘  
1. 병렬 환경에서 총 $$N$$명의 액터가 길이 $$T$$의 궤적을 수집  
2. 어드밴티지 추정(Generalized Advantage Estimation)  
3. 클리핑 대리 목적함수 $$L^{\mathrm{CLIP}}+VF+S$$를 미니배치 SGD(또는 Adam)로 **K 에폭** 학습  
4. $$\theta_{\mathrm{old}}\leftarrow\theta$$ 갱신 후 반복[1]

***

## 3. 모델 구조  
- **정책 신경망**: 완전연결 MLP, 은닉층 2개(각 64유닛, tanh)  
- **출력**: 행동분포의 평균, 가변 표준편차를 별도 파라미터로 관리  
- **가치함수**: 별도 헤드로 MLP 출력, MSE 손실 추가  
- **엔트로피 보너스**: 충분한 탐색을 위해 목적함수에 가중치 $$c_2S[\pi_\theta]$$로 삽입[1]

***

## 4. 성능 향상 및 한계  

### 4.1 성능 비교  
- **연속제어(MuJoCo) 벤치마크**:  
  클리핑 $$\varepsilon=0.2$$ 설정 시, 평균 정규화 점수 0.82로 가장 우수[1]
- **3D 휴머노이드 로코모션**: 복잡 환경에서 안정적 방어 성능 및 학습 속도 우수  
- **Atari 게임**: 전체 학습 기간 평균 보상, 말기 성능 모두 A2C·ACER 대비 다수 게임 우위  

### 4.2 한계  
- **하이퍼파라미터 민감도**: $$\varepsilon$$, 학습률, 에폭 수 등에 따라 성능 편차 존재  
- **극단적 환경 일반화**: 극히 높은 차원 환경(예: 최적화된 물리 시뮬레이터 외부)에서 추가 연구 필요  
- **정책 공유 구조**: 정책·가치 네트워크 파라미터 공유 시 내부 상호작용 영향 분석 부족  

***

## 5. 일반화 성능 향상 관점  
- **클리핑**으로 과도한 정책 변화 억제 → 소량 데이터에서도 **안정적 학습**  
- **다수 에폭 업데이트** 활용 → 샘플 이용 효율 극대화  
- **엔트로피 보너스** 병합 → **다양한 상태**에서 탐색 강화  
- 이러한 요소들이 **다양한 환경**(연속·이산·불안정 시뮬레이터)에서의 **일반화 능력**을 높이는 핵심 메커니즘이다.[1]

***

## 6. 향후 연구에 미치는 영향 및 고려사항  
PPO는 **단순 구현**과 **뛰어난 성능**의 균형으로 인해 강화학습 커뮤니티의 표준 알고리즘이 되었으며, 다음과 같은 연구 방향을 촉진한다.  
- **하이퍼파라미터 자동화**: $$\varepsilon$$ 및 학습률에 대한 메타학습·적응적 조정  
- **모델 기반 강화학습 연계**: 클리핑 방식과 모델 예측 활용 결합  
- **멀티태스크·다중 에이전트 환경**에서의 PPO 확장 및 공정성·안정성 연구  
- **오프라인 데이터 활용**: 클리핑 하한을 활용한 안정적 오프라인 정책 업데이트 메커니즘  

이상과 같은 고려사항은 PPO의 **일반화 성능**과 **실사용 가능성**을 더욱 높이는 핵심 토대가 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9a5f1ce9-7d75-48b3-b44f-e1d4fe195753/1707.06347v2.pdf)
