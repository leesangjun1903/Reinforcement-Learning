# Multi-task Deep Reinforcement Learning with PopArt

**핵심 주장 및 주요 기여**  
“Multi-task Deep Reinforcement Learning with PopArt” 논문은 **PopArt 정규화** 기법을 Actor-Critic 업데이트에 적용해, **보상 규모·희소성·학습 진척도**에 무관한 **스케일 불변(scale-invariant) 업데이트**를 구현함으로써, **하나의 에이전트가 57개의 Atari 게임 및 30개의 DeepMind Lab 레벨**을 동시에 학습해 인간 중간 수준 이상의 성능을 달성하도록 한 연구이다.[1]

***

## 1. 문제 정의  
기존 Deep RL 에이전트는 **단일 과제 학습**에 초점이 맞춰져 있어, 과제마다 별도 에이전트를 훈련해야 했다. 이는 자원 비효율적이며, **다중 과제(multi-task) 설정 시 보상 스케일 간 불균형**으로 특정 과제에 치우친 업데이트가 발생해 일반화 성능이 저하되는 문제를 초래한다.[1]

***

## 2. 제안 방법  
PopArt 정규화(van Hasselt et al. 2016)를 Actor-Critic 업데이트에 확장해, 각 과제별 보상 분포의 평균 μ와 표준편차 σ를 온라인으로 추정하고,  

- **정규화된 가치 예측**:  

$$
    n(s) = \frac{G_t^v - \mu}{\sigma}
  $$

- **스케일 불변 업데이트**:  

$$
    \Delta_\theta \propto (n(s_t) - f_\theta(s_t))\,\nabla_\theta f_\theta(s_t),
    \quad
    \Delta_\eta \propto n(s_t)\,\nabla_\eta \log\pi_\eta(a_t|s_t)
  $$

- **출력 보존(output-preserving) 조정**으로, μ·σ 갱신 시에도 비정규화된 값 예측 $$v(s)$$이 변하지 않도록 최종 레이어 가중치 W, 편향 b를 재조정.[1]

이를 **다중 과제**에 적용해 과제별 $μ_i, σ_i$ 를 별도로 유지하고, 각 과제 가치 헤드만 업데이트하면서 정책 네트워크는 과제 비의존적으로 학습한다.[1]

***

## 3. 모델 구조  
- **IMPALA** 기반 아키텍처: CNN + LSTM ⟶ 정책·가치 헤드  
- **PopArt 확장**: 가치 함수 마지막 레이어에 과제별 $μ_i·σ_i$ 추정 모듈 추가  
- **병렬 학습**: 다수의 CPU 액터(actor)가 데이터를 생성하고, 단일 GPU 학습자(learner)가 중요도 가중치(v-trace) 보정 후 업데이트 수행.[1]

***

## 4. 성능 향상  
- **Atari-57 (보상 클리핑)**: PopArt-IMPALA는 단일 에이전트로 **110%**의 중간 인간 정규화 점수를 달성해, IMPALA(≈60%) 대비 두 배 이상 성능 향상.[1]
- **Atari-57 (비클리핑)**: PopArt-IMPALA는 **101%**, IMPALA는 **0.3%**에 머무름.[1]
- **DmLab-30**: PopArt-IMPALA는 평균 **72.8%**(기존 49.4%)를 기록하며, 원본 IMPALA 및 방대한 액션 세트 확장 버전을 모두 능가.[1]

이러한 성능 향상은 **보상 스케일 불균형으로 인한 업데이트 소실 문제**를 효과적으로 해결했기 때문이다.

***

## 5. 한계 및 일반화 성능  
PopArt-IMPALA는 **강력한 일반화**를 보이지만,  
- **병렬 환경 의존성**: 대규모 병렬 시뮬레이션이 필수이며, 현실 로봇에 적용 시 자원 제약이 발생할 수 있다.  
- **통신·동기화 비용**: 액터-학습자 간 중요도 가중치 동기화로 인한 오버헤드 존재.  
- **하이퍼파라미터 민감도**: μ·σ 업데이트 감쇠율과 출력 보존 클리핑 범위 설정에 주의 필요.[1]

그럼에도 **과제별 $μ_i·σ_i$ 를 추정**해 업데이트를 보정하는 PopArt 정규화가, **다양한 과제**에서 **정상 분포 가정 없이** 일반화 성능을 높일 수 있음을 시사한다.

***

## 6. 향후 연구에 미치는 영향 및 고려 사항  
향후 연구에서는 PopArt-IMPALA와의 **조합**을 통해 더욱 발전시킬 수 있다:  
- **정책 증류(policy distillation)** 및 **능동 샘플링(active sampling)** 으로 데이터 효율성 제고  
- **Elastic Weight Consolidation** 등 **연속 학습(continual learning)** 기법 적용해 과제 간 간섭 최소화  
- **현실 환경 병렬화** 연구로 로봇 제어 등 실세계 적용 범위 확장  

이때 **PopArt 정규화의 μ·σ 적응**과 **다중 과제 학습 스케일링** 특성을 고려해, **하이퍼파라미터 자동 튜닝** 및 **경량화** 방안을 병행 연구하는 것이 유망하다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ef5b038b-a937-4075-ad60-fd8b9fddb19d/1809.04474v1.pdf)
