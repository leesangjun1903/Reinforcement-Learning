# TAMER: Training an Agent Manually via Evaluative Reinforcement

## 핵심 주장과 주요 기여

TAMER 논문의 핵심 주장은 **인간이 단순한 스칼라 보상 신호만으로 학습 에이전트를 효과적으로 훈련할 수 있다**는 것입니다. 주요 기여는 다음과 같습니다:[1]

- **직관적인 인간-AI 상호작용 프레임워크** 제안: 복잡한 프로그래밍 지식 없이 긍정/부정 피드백만으로 에이전트 훈련 가능[1]
- **학습 속도의 획기적 향상**: 테트리스 도메인에서 기존 자율 학습 대비 **10배 이상 빠른 학습** 달성[1]
- **일반화 가능한 프레임워크**: MDP\R (reward function이 제거된 MDP) 환경에서 작동하는 범용 접근법[1]

## 해결하고자 하는 문제

### 기존 강화학습의 한계
- **긴 학습 시간**: 물리적 로봇의 경우 하드웨어 마모, 고위험 환경에서의 경제적 손실[1]
- **전문 지식 요구**: 기존 지식 전달 방법들이 프로그래밍 언어나 복잡한 인터페이스 요구[1]
- **환경 보상 함수 정의의 어려움**: 다중 목표 환경이나 인간 선호도 반영이 어려운 상황[1]

## 제안하는 방법

### TAMER 프레임워크
TAMER는 인간 트레이너가 에이전트의 행동을 관찰하고 스칼라 보상 신호를 제공하여 에이전트를 훈련시키는 시스템입니다.[1]

### 핵심 알고리즘 구성요소

**1. 보상 모델 업데이트 (UpdateRewardModel)**

선형 함수 근사기를 사용한 gradient descent:

$$
\text{error} = r_{t-2} - \sum_i (w_i \times \Delta f_{t-1,t-2,i})
$$

$$
w_i \leftarrow w_i + \alpha \times \text{error} \times \Delta f_{t-1,t-2,i}
$$

여기서 $$\Delta f_{t-1,t-2}$$는 상태 변화를 나타내는 delta-features입니다.[1]

**2. 행동 선택 (ChooseAction)**

각 가능한 행동 $$a$$에 대해 예상 보상을 계산:

$$
\text{projectedRew}\_a = \sum_i (w_i \times \Delta f_{t+1,t,i})
$$

$$
a^* = \arg\max_a(\text{projectedRew}_a)
$$

**3. Delta-Features 표현**

행동을 상태 변화로 표현하는 핵심 아이디어:

$$
\Delta f_{t+1,t} = f_{t+1} - f_t
$$

이는 연속/이산 행동 공간을 통일된 방식으로 처리할 수 있게 합니다.[1]

## 모델 구조

### 아키텍처 특징
- **선형 함수 근사기**: 인간 보상 함수를 모델링하는 핵심 구성요소[1]
- **탐욕적 정책**: 학습된 모델에 따라 즉시 보상 최대화 (장기 할인 보상 아님)[1]
- **지도학습 접근법**: 각 행동을 훈련 샘플로 취급, 상태 특징을 속성, 인간 보상을 레이블로 사용[1]

### 주요 가정
1. **결정론적 환경** (또는 결정론적 afterstate 존재)[1]
2. **충분한 피드백 시간**: 행동 간 인간이 피드백을 제공할 시간 확보[1]

## 성능 향상 결과

### 테트리스 실험 결과
- **3번째 게임에서 평균 65.89줄 클리어** 달성[1]
- 기존 최고 성능 자율 학습 대비:
  - RRL-KBR: 120게임 후 50줄 (24배 빠름)[1]
  - Policy Iteration: 1500게임 후 3,183줄 (500배 빠른 학습 시작)[1]
  - Genetic Algorithm: 3000게임 후 586,103줄[1]

### 일반화 성능
- **기술적 배경이 없는 사용자**도 AI 전문가와 유사한 훈련 성과 달성[1]
- **최소한의 지시사항**으로 효과적인 훈련 가능[1]

## 한계점

### 성능 상한선
- **최고 성능은 기존 알고리즘 대비 낮음**: 테트리스에서 수십만 줄 클리어하는 다른 알고리즘 대비 제한적[1]
- **상수 스텝 사이즈 α의 한계**: 증분 업데이트 방식의 구조적 제약[1]

### 환경 제약
- **결정론적 환경 가정**: 확률적 환경에서는 확장 필요[1]
- **실시간 피드백 요구**: 빠른 행동 시퀀스가 필요한 태스크에서는 적용 어려움[1]

### 인간 보상 함수의 불일치성
- **이동하는 목표**: 성능 향상에 따라 인간의 기준이 변화하는 "shaping" 문제[1]
- **일관성 부족**: 인간 보상 함수의 본질적 불일치성[1]

## 모델의 일반화 성능 향상 가능성

### 확장성 방향
1. **비결정론적 환경**: 전이 함수가 확률 분포를 반환하는 경우, 가중 합으로 특징 벡터 계산 가능[1]
2. **실시간 환경**: Eligibility traces 사용하여 이전 행동들에 지수적 감쇠 적용[1]
3. **하이브리드 학습**: 인간 피드백과 환경 보상을 결합한 학습[1]

### 일반화 장점
- **도메인 독립적**: MDP\R 프레임워크로 다양한 태스크 적용 가능[1]
- **사용자 친화적**: 기술적 배경 불필요, 직관적 인터페이스[1]
- **빠른 부트스트래핑**: 초기 정책 학습에 매우 효과적[1]

## 향후 연구에 미치는 영향

### 긍정적 영향
- **Human-in-the-loop 학습의 선구적 연구**: 현재 RLHF(Reinforcement Learning from Human Feedback)의 이론적 기초 제공
- **실용적 AI 시스템 개발**: 비전문가도 AI 시스템을 쉽게 훈련할 수 있는 패러다임 제시
- **안전한 AI 학습**: 인간 감독 하에 안전하고 예측 가능한 학습 과정

### 향후 연구 고려사항

**방법론적 개선**
- **더 정교한 함수 근사기** 도입: 딥러닝 기반 모델로 확장
- **온라인 학습 안정성** 향상: 적응적 학습률, 정규화 기법 도입
- **다중 인간 트레이너** 처리: 불일치하는 피드백 통합 방법

**실용적 확장**
- **복잡한 실제 환경** 적용: 로봇 제어, 자동차 운전 등
- **장기 메모리 통합**: 과거 학습 경험 활용 메커니즘
- **설명 가능한 AI**: 학습된 정책의 해석성 향상

**이론적 발전**
- **수렴성 보장**: 이론적 수렴 조건 분석
- **샘플 복잡도**: 필요한 인간 피드백 양에 대한 이론적 분석
- **로버스트성**: 노이즈가 있는 인간 피드백에 대한 강건성

TAMER는 현재 ChatGPT, GPT-4 등에서 활용되는 RLHF 기술의 이론적 토대를 마련한 선구적 연구로, 인간과 AI의 협력적 학습 패러다임을 제시한 중요한 기여를 하였습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/1c8e43e3-31bf-4883-be82-e11d79d82f88/icdl08-knox.pdf)
