# Monte-Carlo Tree Search(MCTS)란 무엇인가?

Monte-Carlo Tree Search(MCTS)는 **확률적 시뮬레이션**과 **트리 탐색**을 결합하여, 특히 **게임 인공지능** 분야에서 강력한 성능을 발휘하는 탐색 알고리즘입니다. 복잡한 상태 공간에서도 연산 자원을 효율적으로 분배하며 최적의 수를 찾아낼 수 있다는 장점이 있습니다.

***

## 1. MCTS의 기본 아이디어

1. **무작위 시뮬레이션**  
   현재 상태(State)에서 가능한 수들을 임의로 뽑아, 게임이 종료될 때까지 **플레이아웃(play-out)**을 수행합니다.  
   → 시뮬레이션 결과(승리·패배·무승부)를 얻음.

2. **트리 구조 유지**  
   각 상태를 노드(node)로, 가능한 수(액션)를 간선(edge)으로 표현하여 **탐색 트리**를 구성합니다.

3. **탐색과 학습의 반복**  
   수천에서 수만 회의 시뮬레이션을 반복하며, 트리를 점진적으로 확장(expansion)하고, 시뮬레이션 결과를 역전파(back-propagation)하여 각 간선의 **가치(value)**와 **방문 횟수(visit count)**를 갱신합니다.

***

## 2. MCTS의 네 가지 단계

1. **선택(Selection)**  
   루트(root)에서 시작해, **UCT(UCB1)** 등 선택 기준에 따라 자식 노드로 내려갑니다.  
   이때 각 액션 $$a$$에 대해 다음 공식을 사용해 선택합니다:  

$$
     UCT(a) = \bar{X}_a + C \sqrt{\frac{\ln N}{n_a}}
   $$

   – $$\bar{X}_a$$: 액션 $$a$$의 평균 보상(가치)  
   – $$N$$: 현재 노드의 방문 횟수  
   – $$n_a$$: 액션 $$a$$로 이동한 자식 노드의 방문 횟수  
   – $$C$$: 탐색(exploitation)과 탐험(exploration) 간 균형을 조절하는 상수  

2. **확장(Expansion)**  
   도달한 노드가 **완전히 확장**되지 않았다면, 가능한 수 중 하나를 새 노드로 추가합니다.

3. **시뮬레이션(Simulation, Play-out)**  
   확장된 새 노드에서 무작위(또는 휴리스틱) 방식으로 게임을 끝까지 플레이하여 결과(승·패·무)를 얻습니다.

4. **역전파(Back-propagation)**  
   시뮬레이션 결과를 트리의 상위 노드들로 거슬러 올라가면서, 방문 횟수 $$n$$와 누적 보상 $$w$$를 다음과 같이 갱신합니다.  
   – 방문 횟수: $$n \leftarrow n + 1$$  
   – 누적 보상: $$w \leftarrow w + \text{시뮬레이션 보상}$$

이 과정을 예산(시간 또는 반복 횟수)이 소진될 때까지 반복하고, 루트 노드에서 가장 높은 방문 횟수를 가진 액션을 선택하여 실제 수로 두게 됩니다.

***

## 3. MCTS의 장점과 한계

### 장점
- **도메인 지식 최소화**  
  전통적 게임 트리 탐색(미니맥스)처럼 복잡한 평가 함수를 요구하지 않음.
- **점진적 개선**  
  시뮬레이션 횟수를 늘릴수록 성능이 향상되어, 시간 제약 조건에 유연하게 대응.
- **복잡도 관리**  
  유망한 수만 집중적으로 탐색하고, 중요하지 않은 수는 적게 탐색.

### 한계
- **수렴 속도**  
  매우 깊거나 분기가 큰 게임에서는 충분한 시뮬레이션을 요구.  
- **무작위성 의존**  
  순수 무작위 플레이아웃은 복잡한 전략적 상황을 잘 반영하지 못해 성능 저하 가능.  
- **메모리 사용량**  
  대규모 트리 구조를 유지하므로 메모리 요구량이 커질 수 있음.

***

## 4. MCTS의 실용적 변형

- **UCT 개선**: UCB1GRAVE, Progressive History, UCB1Tuned 등  
- **휴리스틱 통합**: MAST, NST, 평가 함수(state evaluator)  
- **병렬화**: 트리 병렬, 루트 병렬 기법으로 속도 향상  

이러한 변형은 **탐색 전략**, **플레이아웃 정책**, **탐색-탐험 균형** 등에서 각기 다른 장단점을 가지며, 게임 특성에 맞춰 선택해야 합니다.

***

### 요약

MCTS는 **무작위 시뮬레이션**과 **확률적 탐색**을 결합하여 복잡한 게임에서도 유망한 수를 찾아내는 강력한 알고리즘입니다. 간결한 핵심 원리와 도메인 지식 최소화라는 장점 덕분에 바둑, 체스, 일반 게임 플레이, 최적화 문제 등 다양한 분야에서 폭넓게 활용됩니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/903f4e83-6930-4556-a1d7-fa8e9249fd7f/MCTS.pdf

# 핵심 주장 및 주요 기여 요약

**핵심 주장**  
본 논문은 서로 다른 1,494개의 게임에서 61종의 에이전트를 활용해 수행된 268,386회 시뮬레이션 결과를 대규모 데이터셋으로 구축하고, 이를 통해 다양한 Monte-Carlo Tree Search(MCTS) 변형 알고리즘이 어떤 게임 유형에서 우수하거나 취약한지 체계적으로 분석하고 예측할 수 있음을 입증한다.

**주요 기여**  
1. **대규모 MCTS 비교 데이터셋 구축**  
   – 60종의 MCTS 변형(선택 전략 4종 × 탐색 상수 3종 × 플레이아웃 전략 5종)과 랜덤 에이전트 1종, 총 61개 에이전트  
   – Ludii 프레임워크 내 1,494개 게임, 각 게임을 809개의 메타·플레이아웃 특성으로 표현  
   – 복수의 대전 매치업(2인 제로섬, 다인·싱글톤)에서 총 268,386회 플레이 실행  

2. **데이터 정제 및 예측 문제 설정**  
   – 2인 제로섬 게임으로 한정하여, 에이전트 쌍과 게임 특성을 입력으로 첫 번째 에이전트의 기대 유틸리티를 예측하는 회귀 문제 정의  
   – 랜덤 에이전트 매치업, 셀프 매치업(미러 매치) 제거 후 118,205회 플레이 남김  
   – 특성 차원 축소(고정·결측 특성 제거), 에이전트 구성 변수 원-핫 인코딩  

3. **예비 예측 모델 및 해석**  
   – Dummy, 결정 트리, 랜덤 포레스트 회귀모델 비교  
   – 1,376-겹 교차검증에서 Random Forest가 RMSE 0.491±0.258, MAE 0.434±0.248로 최고 성능 달성 (Table II)  
   – SHAP 분석을 통해 “랜덤 플레이아웃으로 측정된 1P 우위(AdvantageP1)”가 가장 강력한 예측 인자임을 확인, 단순 랜덤 예측 우위가 MCTS 성능 예측에 유효함을 시사 (Fig. 1)  

# 자세한 설명

## 1. 해결하고자 하는 문제  
- **배경**: MCTS는 일반 게임 플레이와 AI 게임 플레이 분야에서 광범위하게 쓰이지만, 다양한 변형들이 어떤 게임 특성에 잘 맞는지 체계적 이해는 부족하다.  
- **문제**: 개별 연구마다 실험에 사용하는 게임 수가 제한적(10–30종)이고, 게임 속성별 MCTS 성능 차이를 일반화하기 어렵다.  
- **목표**: 대규모 게임·에이전트 조합 데이터로 학습 가능한 예측 모델을 구축해, 게임 속성(feature)과 MCTS 변형 간 상관관계를 밝혀내고, 어떤 변형이 어떤 게임에서 유리한지 예측.

## 2. 제안하는 방법  

### 2.1 데이터셋 구성  
- Ludii 프레임워크 기반 1,494개 게임  
- 각 게임당 809개 특성(features) 추출(이진·수치·플레이아웃 개념 포함)  
- 61개 에이전트:  
  - 선택(selection) 전략: UCB1, UCB1GRAVE, Progressive History, UCB1Tuned  
  - 탐색 상수 $$C \in \{0.1,\,0.6,\,\sqrt2\}$$  
  - 플레이아웃(play-out) 전략: Random0, Random4, Random200, MAST, NST  
- 매치업 구성:  
  - 1인 게임: MCTS 에이전트 단독 플레이  
  - $$k$$-인 게임: MCTS 대 랜덤 × $$(k-1)$$, MCTS 대 무작위 MCTS × $$(k-1)$$  
  - 각 매치업 10×k회 반복  

### 2.2 데이터 정제 및 학습 문제 정의  
- 제로섬 2인 게임만 선별, 랜덤 및 미러 매치업 제거 → 118,205회 플레이  
- 특성 차원: 809개 → 불변·결측 특성 제거 → 최종 545개(추정)  
- 입력:  

```math
    \bigl(\underbrace{\text{game\_features}_{1\times d}}_{\text{809-D}}\,,\;
    \text{agent1\_strategy},\,\text{agent1\_C},\,\text{agent1\_playout},\;
    \text{agent2\_…}
    \bigr)
```

- 목표(target): 첫 번째 에이전트의 기대 유틸리티 $$u_1\in[-1,1]$$  
- 제로섬 가정: $$u_2=-u_1$$

### 2.3 모델 구조 및 학습  
- **Dummy Regressor**: 모든 입력에 대해 평균값 $$\bar u_1$$ 예측  
- **Decision Tree Regressor**: 최대 깊이 10  
- **Random Forest Regressor**: 평균 10그루, 최대 깊이 10  
- 학습·평가: 1,376겹 교차검증(게임별 폴드 분리)  

## 3. 성능 향상 및 해석 결과  

| 모델              | RMSE (± std)       | MAE (± std)       |
|-------------------|--------------------|-------------------|
| Dummy             | 0.640 (± 0.267)    | 0.585 (± 0.276)   |
| Decision Tree     | 0.544 (± 0.307)    | 0.465 (± 0.290)   |
| Random Forest     | 0.491 (± 0.258)    | 0.434 (± 0.248)   |  
Table II: 교차검증 성능. Random Forest가 가장 낮은 오류 달성.  

- **SHAP 분석** (Fig. 1)  
  1. **AdvantageP1** (랜덤 플레이아웃 기반 1P 우위)  
  2. **Play-out1/2_Random4**, **Random0**: 조기 종료 랜덤 플레이아웃 전략 사용 시 성능 저하  
  3. 기타 복합 특성 및 나머지 556개 특성  

**인사이트**:  
- 단순 무작위로 추산한 플레이아웃 우위가 MCTS 대전 결과 예측에 유의미함.  
- 조기 종료 랜덤 플레이아웃 전략은 상태 평가 함수 부재 시 비효율적임.  

## 4. 한계 및 향후 과제  
1. **데이터 희소성**: 61×61 모든 에이전트 조합·게임 조합 시뮬레이션 불가능 → 표본의 약 10%만 수행  
2. **모델 단순성**: 결정 트리와 랜덤 포레스트만 사용, 심층 신경망·그래프 기반 모델 미적용  
3. **게임 특성**: Ludii에서 제공하는 809개 특성 외에도, 전문가 휴리스틱·심층 학습 피처 미포함  
4. **다인·비제로섬 게임 배제**: 2인 제로섬으로 범위 제한  
5. **플레이아웃 전략 한계**: MAST·NST만 사용, 최신 정책 네트워크 기반 시뮬레이션 부재  

**향후 연구 방향**  
- 심층 모델(딥러닝, 그래프 신경망) 적용  
- 다인·비제로섬 게임까지 범위 확장  
- 플레이아웃 전략 다변화 및 상태 평가 함수 통합  
- 게임 특성에 심층 학습으로부터 추출한 표현(rep­resentation) 추가  

---  
본 연구는 MCTS 변형 알고리즘 간 성능 차이를 게임 특성별로 예측·설명할 수 있는 데이터셋과 예비 모델을 제시함으로써, 다양한 게임에서의 MCTS 적용 가이드라인 수립을 위한 초석을 마련하였다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/903f4e83-6930-4556-a1d7-fa8e9249fd7f/MCTS.pdf
